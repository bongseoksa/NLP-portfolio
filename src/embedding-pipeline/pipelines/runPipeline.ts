import fs from "fs";
import path from "path";
import { fetchAllCommits } from "../data_sources/github/fetchCommit.js";
import { fetchFiles } from "../data_sources/github/fetchFiles.js";
import { fetchRepositoryFiles } from "../data_sources/github/fetchRepositoryFiles.js";
import type { PipelineOutput } from "../../shared/models/PipelineOutput.js";
import { refineData } from "./steps/preprocessText.js";
import { generateEmbeddings } from "../nlp/embedding/openaiEmbedding.js";
import { saveVectors } from "../storage/saveVectors.js";
import { saveVectorsSupabase } from "../storage/saveVectorsSupabase.js";
import type { EmbeddingItem } from "../../shared/models/EmbeddingItem.js";

export interface PipelineOptions {
    /** ê¸°ì¡´ ë²¡í„° ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„± (ì„ë² ë”© ì°¨ì› ë³€ê²½ ì‹œ í•„ìš”) */
    reset?: boolean;
    /** íŠ¹ì • ë ˆí¬ì§€í† ë¦¬ ì§€ì • (owner/repo í˜•ì‹) */
    targetRepo?: { owner: string; repo: string };
    /** Supabase Vector Store ì‚¬ìš© (í™˜ê²½ ë³€ìˆ˜ë¡œë„ ì œì–´ ê°€ëŠ¥) */
    useSupabase?: boolean;
}

/**
 * ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
 * 1. GitHub API ì»¤ë°‹ ìˆ˜ì§‘
 * 2. ë³€ê²½ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ (GitHub API - patch í¬í•¨)
 * 3. ë ˆí¬ì§€í† ë¦¬ ì†ŒìŠ¤ ì½”ë“œ ìˆ˜ì§‘
 * 4. ë°ì´í„° ì •ì œ (NLP ì…ë ¥ í˜•íƒœ)
 * 5. ì„ë² ë”© ìƒì„± (OpenAI â†’ Chroma ê¸°ë³¸ ì„ë² ë”© fallback)
 * 6. ë²¡í„° ì €ì¥ (Chroma)
 */
export async function runPipeline(options: PipelineOptions = {}) {
    const { reset = false, targetRepo, useSupabase: optionUseSupabase } = options;

    // Supabase ì‚¬ìš© ì—¬ë¶€ ê²°ì •: ì˜µì…˜ > í™˜ê²½ ë³€ìˆ˜
    const useSupabase = optionUseSupabase ?? (process.env.SUPABASE_URL && process.env.SUPABASE_SERVICE_ROLE_KEY) ? true : false;

    console.log("ğŸš€ Pipeline started\n");
    console.log(`ğŸ“Š Vector Store: ${useSupabase ? "Supabase (Cloud)" : "ChromaDB (Local)"}`);

    if (reset) {
        console.log("ğŸ”„ Reset mode enabled: Vector collection will be recreated.\n");
    }

    // targetRepo ì˜µì…˜ í•„ìˆ˜ (target-repos.jsonì€ runPollingPipelineì—ì„œ ì²˜ë¦¬)
    if (!targetRepo) {
        console.error("âŒ targetRepo ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤. runPollingPipelineì„ ì‚¬ìš©í•˜ê±°ë‚˜ targetRepo ì˜µì…˜ì„ ì œê³µí•´ì£¼ì„¸ìš”.");
        return;
    }

    const { owner, repo } = targetRepo;

    console.log(`ğŸ“¦ Target repository: ${owner}/${repo}`);

    const outputDir = path.join(process.cwd(), "output");
    if (!fs.existsSync(outputDir)) fs.mkdirSync(outputDir);

    // ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì´ì „ ì„ë² ë”© íŒŒì¼ ì¡°íšŒ ë¡œì§ ì œê±°)
    const result: PipelineOutput = {
        commits: [],
        commitFiles: {},
        repositoryFiles: []
    };

        // 1ï¸âƒ£ GitHub ì»¤ë°‹ ì „ì²´ ê°€ì ¸ì˜¤ê¸°
        console.log("ğŸ“Œ Fetching commit list from GitHub...");
        const commits = await fetchAllCommits(owner, repo);
        result.commits = commits;
        console.log(`   â†’ ${commits.length} commits fetched.`);

        // 2ï¸âƒ£ ê° ì»¤ë°‹ SHAì— ëŒ€í•œ ë³€ê²½ íŒŒì¼ ê°€ì ¸ì˜¤ê¸° (GitHub API - patch í¬í•¨)
        console.log("\nğŸ“Œ Fetching changed files for each commit (with patch)...");
        for (const commit of commits) {
            const sha = commit.sha;
            const files = await fetchFiles({ owner, repo, sha });
            result.commitFiles[sha] = files;
        }
        console.log("   â†’ commitFiles completed.");

        // 3ï¸âƒ£ ë ˆí¬ì§€í† ë¦¬ ëª¨ë“  íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì†ŒìŠ¤ ì½”ë“œ ë ˆë²¨ ì§ˆë¬¸ìš©)
        console.log("\nğŸ“Œ Fetching repository files (source code)...");
        try {
            // ê¸°ë³¸ ë¸Œëœì¹˜ ìë™ ê°ì§€ (null ì „ë‹¬ ì‹œ ìë™ìœ¼ë¡œ ê¸°ë³¸ ë¸Œëœì¹˜ ì‚¬ìš©)
            const repositoryFiles = await fetchRepositoryFiles(owner, repo, null, {
                maxFileSize: 500000, // 500KB
                excludeExtensions: ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.woff', '.woff2', '.ttf', '.eot', '.pdf'],
                excludePaths: ['node_modules', '.git', 'dist', 'build', '.next', '.venv', '__pycache__', '.chroma_venv'],
                concurrency: 5, // ë™ì‹œ ìš”ì²­ ìˆ˜
            });
            result.repositoryFiles = repositoryFiles;
            console.log(`   â†’ ${repositoryFiles.length} files fetched.`);
        } catch (error: any) {
            console.error("âŒ Repository files fetch failed:", error.message);
            console.warn("   â†’ Continuing without repository files...");
            result.repositoryFiles = [];
        }

        // 4ï¸âƒ£ JSON íŒŒì¼ë¡œ ì €ì¥ (Raw)
        fs.writeFileSync(
            path.join(outputDir, "pipeline_output.json"),
            JSON.stringify(result, null, 2),
            "utf-8"
        );

        // 5ï¸âƒ£ ë°ì´í„° ì •ì œ
        console.log("\nğŸ“Œ Data Refinement (NLP Preparation)...");
        const refinedData = refineData(result);
        console.log(`   â†’ ${refinedData.items.length} items refined.`);

        // 6ï¸âƒ£ ì„ë² ë”© ìƒì„± ë° ì €ì¥ (OpenAI ë˜ëŠ” Chroma ê¸°ë³¸ ì„ë² ë”© fallback)
        console.log("\nğŸ“Œ Generating Embeddings...");
        try {
            const batchSize = 10;
            const items = refinedData.items;
        const embeddings: number[][] = [];

        // Batch processing to avoid huge payload
        for (let i = 0; i < items.length; i += batchSize) {
            const batch = items.slice(i, i + batchSize);
            // Use embeddingText (ìì—°ì–´ ë³€í™˜) instead of content (raw data)
            const texts = batch.map((item: any) => item.embeddingText);
            console.log(`   Processing batch ${i / batchSize + 1}/${Math.ceil(items.length / batchSize)}...`);

            const batchEmbeddings = await generateEmbeddings(texts);
            embeddings.push(...batchEmbeddings);
        }

        console.log(`   â†’ Generated ${embeddings.length} vectors.`);

        // ë²¡í„° ì €ì¥ (Supabase or ChromaDB)
        if (useSupabase) {
            console.log("\nğŸ“Œ Saving to Supabase Vector Store...");

            // EmbeddingItem í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            const embeddingItems: EmbeddingItem[] = items.map((item: any, idx: number) => ({
                id: item.id,
                content: item.content,
                embedding: embeddings[idx] || [],
                metadata: {
                    ...item.metadata,
                    owner,
                    repo
                }
            }));

            await saveVectorsSupabase(embeddingItems, { reset, owner, repo });
        } else {
            console.log("\nğŸ“Œ Saving to ChromaDB...");
            // Collection name: ëª¨ë“  íƒ€ì…(commit, diff, file)ì„ í•˜ë‚˜ì˜ ì»¬ë ‰ì…˜ì— ì €ì¥
            // ë©”íƒ€ë°ì´í„°ì˜ type í•„ë“œë¡œ êµ¬ë¶„ë¨
            await saveVectors(`${repo}-vectors`, items, embeddings, reset);
        }

    } catch (err: any) {
        console.error("âŒ Embedding/Vector Store Failed:", err.message);
        if (!useSupabase) {
            console.error("   (Is ChromaDB running? 'pnpm run chroma:start')");
        }
    }

    console.log("\nğŸ‰ Pipeline finished!");
    console.log("ğŸ“ Saved â†’ output/pipeline_output.json");
}

// ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
// ESM pattern to check if file is run directly
import { fileURLToPath } from "url";

if (process.argv[1] === fileURLToPath(import.meta.url)) {
    runPipeline().catch(err => {
        console.error("âŒ Pipeline failed:", err);
        process.exit(1);
    });
}
