import fs from "fs";
import path from "path";
import { fetchAllCommits } from "../data_sources/github/fetchCommit.js";
import { fetchFiles } from "../data_sources/github/fetchFiles.js";
import { fetchRepositoryFiles } from "../data_sources/github/fetchRepositoryFiles.js";
import type { PipelineOutput } from "../models/PipelineOutput.js";
import { refineData } from "./steps/preprocessText.js";
import { generateEmbeddings } from "../nlp/embedding/openaiEmbedding.js";
import { saveVectors } from "../vector_store/saveVectors.js";

export interface PipelineOptions {
    /** ê¸°ì¡´ ë²¡í„° ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„± (ì„ë² ë”© ì°¨ì› ë³€ê²½ ì‹œ í•„ìš”) */
    reset?: boolean;
    /** ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ì¬ì„ë² ë”©ë§Œ ìˆ˜í–‰) */
    skipFetch?: boolean;
}

/**
 * ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
 * 1. GitHub API ì»¤ë°‹ ìˆ˜ì§‘
 * 2. ë³€ê²½ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ (GitHub API - patch í¬í•¨)
 * 3. ë ˆí¬ì§€í† ë¦¬ ì†ŒìŠ¤ ì½”ë“œ ìˆ˜ì§‘
 * 4. ë°ì´í„° ì •ì œ (NLP ì…ë ¥ í˜•íƒœ)
 * 5. ì„ë² ë”© ìƒì„± (OpenAI â†’ Chroma ê¸°ë³¸ ì„ë² ë”© fallback)
 * 6. ë²¡í„° ì €ì¥ (Chroma)
 */
export async function runPipeline(options: PipelineOptions = {}) {
    const { reset = false, skipFetch = false } = options;
    
    console.log("ğŸš€ Pipeline started\n");
    if (reset) {
        console.log("ğŸ”„ Reset mode enabled: Vector collection will be recreated.\n");
    }

    const owner = process.env.TARGET_REPO_OWNER!;
    const repo = process.env.TARGET_REPO_NAME!;

    if (!owner || !repo) {
        console.error("âŒ TARGET_REPO_OWNER / TARGET_REPO_NAME í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.");
        return;
    }

    const outputDir = path.join(process.cwd(), "output");
    if (!fs.existsSync(outputDir)) fs.mkdirSync(outputDir);

    let result: PipelineOutput;
    let refinedData: { items: any[] };

    if (skipFetch) {
        // ì¬ì„ë² ë”© ëª¨ë“œ: ê¸°ì¡´ refined_data.json ì‚¬ìš©
        console.log("â­ï¸ Skipping data fetch (using existing refined_data.json)...\n");
        
        const refinedPath = path.join(outputDir, "refined_data.json");
        if (!fs.existsSync(refinedPath)) {
            console.error("âŒ refined_data.json not found. Run full pipeline first.");
            return;
        }
        
        refinedData = JSON.parse(fs.readFileSync(refinedPath, "utf-8"));
        console.log(`ğŸ“‚ Loaded ${refinedData.items.length} items from refined_data.json\n`);
        
    } else {
        // ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = {
            commits: [],
            commitFiles: {},
            repositoryFiles: []
        };

        // 1ï¸âƒ£ GitHub ì»¤ë°‹ ì „ì²´ ê°€ì ¸ì˜¤ê¸°
        console.log("ğŸ“Œ Fetching commit list from GitHub...");
        const commits = await fetchAllCommits();
        result.commits = commits;
        console.log(`   â†’ ${commits.length} commits fetched.`);

        // 2ï¸âƒ£ ê° ì»¤ë°‹ SHAì— ëŒ€í•œ ë³€ê²½ íŒŒì¼ ê°€ì ¸ì˜¤ê¸° (GitHub API - patch í¬í•¨)
        console.log("\nğŸ“Œ Fetching changed files for each commit (with patch)...");
        for (const commit of commits) {
            const sha = commit.sha;
            const files = await fetchFiles({ owner, repo, sha });
            result.commitFiles[sha] = files;
        }
        console.log("   â†’ commitFiles completed.");

        // 3ï¸âƒ£ ë ˆí¬ì§€í† ë¦¬ ëª¨ë“  íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì†ŒìŠ¤ ì½”ë“œ ë ˆë²¨ ì§ˆë¬¸ìš©)
        console.log("\nğŸ“Œ Fetching repository files (source code)...");
        try {
            // ê¸°ë³¸ ë¸Œëœì¹˜ ìë™ ê°ì§€ (null ì „ë‹¬ ì‹œ ìë™ìœ¼ë¡œ ê¸°ë³¸ ë¸Œëœì¹˜ ì‚¬ìš©)
            const repositoryFiles = await fetchRepositoryFiles(owner, repo, null, {
                maxFileSize: 500000, // 500KB
                excludeExtensions: ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.woff', '.woff2', '.ttf', '.eot', '.pdf'],
                excludePaths: ['node_modules', '.git', 'dist', 'build', '.next', '.venv', '__pycache__', '.chroma_venv'],
                concurrency: 5, // ë™ì‹œ ìš”ì²­ ìˆ˜
            });
            result.repositoryFiles = repositoryFiles;
            console.log(`   â†’ ${repositoryFiles.length} files fetched.`);
        } catch (error: any) {
            console.error("âŒ Repository files fetch failed:", error.message);
            console.warn("   â†’ Continuing without repository files...");
            result.repositoryFiles = [];
        }

        // 4ï¸âƒ£ JSON íŒŒì¼ë¡œ ì €ì¥ (Raw)
        fs.writeFileSync(
            path.join(outputDir, "pipeline_output.json"),
            JSON.stringify(result, null, 2),
            "utf-8"
        );

        // 5ï¸âƒ£ ë°ì´í„° ì •ì œ
        console.log("\nğŸ“Œ Data Refinement (NLP Preparation)...");
        refinedData = refineData(result);
        fs.writeFileSync(
            path.join(outputDir, "refined_data.json"),
            JSON.stringify(refinedData, null, 2),
            "utf-8"
        );
        console.log(`   â†’ ${refinedData.items.length} items refined.`);
    }

    // 6ï¸âƒ£ ì„ë² ë”© ìƒì„± ë° ì €ì¥ (OpenAI ë˜ëŠ” Chroma ê¸°ë³¸ ì„ë² ë”© fallback)
    console.log("\nğŸ“Œ Generating Embeddings...");
    try {
        const batchSize = 10;
        const items = refinedData.items;
        const embeddings: number[][] = [];

        // Batch processing to avoid huge payload
        for (let i = 0; i < items.length; i += batchSize) {
            const batch = items.slice(i, i + batchSize);
            // Use embeddingText (ìì—°ì–´ ë³€í™˜) instead of content (raw data)
            const texts = batch.map((item: any) => item.embeddingText);
            console.log(`   Processing batch ${i / batchSize + 1}/${Math.ceil(items.length / batchSize)}...`);

            const batchEmbeddings = await generateEmbeddings(texts);
            embeddings.push(...batchEmbeddings);
        }

        console.log(`   â†’ Generated ${embeddings.length} vectors.`);

        console.log("\nğŸ“Œ Saving to ChromaDB...");
        // Collection name: ëª¨ë“  íƒ€ì…(commit, diff, file)ì„ í•˜ë‚˜ì˜ ì»¬ë ‰ì…˜ì— ì €ì¥
        // ë©”íƒ€ë°ì´í„°ì˜ type í•„ë“œë¡œ êµ¬ë¶„ë¨
        await saveVectors(`${repo}-vectors`, items, embeddings, reset);

    } catch (err: any) {
        console.error("âŒ Embedding/Vector Store Failed:", err.message);
        console.error("   (Is ChromaDB running? 'pnpm run chroma:start')");
    }

    console.log("\nğŸ‰ Pipeline finished!");
    if (!skipFetch) {
        console.log("ğŸ“ Saved â†’ output/pipeline_output.json");
        console.log("ğŸ“ Saved â†’ output/refined_data.json");
    }
}

// ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
// ESM pattern to check if file is run directly
import { fileURLToPath } from "url";

if (process.argv[1] === fileURLToPath(import.meta.url)) {
    runPipeline().catch(err => {
        console.error("âŒ Pipeline failed:", err);
        process.exit(1);
    });
}
