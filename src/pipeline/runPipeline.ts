import fs from "fs";
import path from "path";
import { fetchAllCommits } from "../data_sources/github/fetchCommit.js";
import { fetchFiles } from "../data_sources/github/fetchFiles.js";
import { parseLog } from "../data_sources/git/parseLog.js";
import { extractDiff } from "../data_sources/git/extractDiff.js";
import type { CommitItem, LocalCommitLog } from "../models/Commit.js";
import type { FileModel } from "../models/File.js";
import type { CommitDiff } from "../models/Diff.js";
import type { PipelineOutput } from "../models/PipelineOutput.js";
import { refineData } from "./steps/preprocessText.js";
import { generateEmbeddings } from "../nlp/embedding/openaiEmbedding.js";
import { saveVectors } from "../vector_store/saveVectors.js";

export interface PipelineOptions {
    /** ê¸°ì¡´ ë²¡í„° ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„± (ì„ë² ë”© ì°¨ì› ë³€ê²½ ì‹œ í•„ìš”) */
    reset?: boolean;
    /** ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ì¬ì„ë² ë”©ë§Œ ìˆ˜í–‰) */
    skipFetch?: boolean;
}

/**
 * ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
 * 1. GitHub API ì»¤ë°‹ ìˆ˜ì§‘
 * 2. ë³€ê²½ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
 * 3. ë¡œì»¬ Git ë¡œê·¸ ë° Diff ì¶”ì¶œ
 * 4. ë°ì´í„° ì •ì œ (NLP ì…ë ¥ í˜•íƒœ)
 * 5. ì„ë² ë”© ìƒì„± (OpenAI â†’ Chroma ê¸°ë³¸ ì„ë² ë”© fallback)
 * 6. ë²¡í„° ì €ì¥ (Chroma)
 */
export async function runPipeline(options: PipelineOptions = {}) {
    const { reset = false, skipFetch = false } = options;
    
    console.log("ğŸš€ Pipeline started\n");
    if (reset) {
        console.log("ğŸ”„ Reset mode enabled: Vector collection will be recreated.\n");
    }

    const owner = process.env.TARGET_REPO_OWNER!;
    const repo = process.env.TARGET_REPO_NAME!;
    const localRepo = process.env.LOCAL_REPO_PATH!;

    if (!owner || !repo) {
        console.error("âŒ TARGET_REPO_OWNER / TARGET_REPO_NAME í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.");
        return;
    }

    if (!localRepo) {
        console.error("âŒ LOCAL_REPO_PATH í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.");
        return;
    }

    const outputDir = path.join(process.cwd(), "output");
    if (!fs.existsSync(outputDir)) fs.mkdirSync(outputDir);

    let result: PipelineOutput;
    let refinedData: { items: any[] };

    if (skipFetch) {
        // ì¬ì„ë² ë”© ëª¨ë“œ: ê¸°ì¡´ refined_data.json ì‚¬ìš©
        console.log("â­ï¸ Skipping data fetch (using existing refined_data.json)...\n");
        
        const refinedPath = path.join(outputDir, "refined_data.json");
        if (!fs.existsSync(refinedPath)) {
            console.error("âŒ refined_data.json not found. Run full pipeline first.");
            return;
        }
        
        refinedData = JSON.parse(fs.readFileSync(refinedPath, "utf-8"));
        console.log(`ğŸ“‚ Loaded ${refinedData.items.length} items from refined_data.json\n`);
        
    } else {
        // ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = {
            commits: [],
            commitFiles: {},
            commitDiffs: [],
            localLogs: []
        };

        // 1ï¸âƒ£ GitHub ì»¤ë°‹ ì „ì²´ ê°€ì ¸ì˜¤ê¸°
        console.log("ğŸ“Œ Fetching commit list from GitHub...");
        const commits = await fetchAllCommits();
        result.commits = commits;
        console.log(`   â†’ ${commits.length} commits fetched.`);

        // 2ï¸âƒ£ ê° ì»¤ë°‹ SHAì— ëŒ€í•œ ë³€ê²½ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        console.log("\nğŸ“Œ Fetching changed files for each commit...");
        for (const commit of commits) {
            const sha = commit.sha;
            const files = await fetchFiles({ owner, repo, sha });
            result.commitFiles[sha] = files;
        }
        console.log("   â†’ commitFiles completed.");

        // 3ï¸âƒ£ ë¡œì»¬ repoì—ì„œ ì»¤ë°‹ diff ê°€ì ¸ì˜¤ê¸°
        console.log("\nğŸ“Œ Extracting local diffs...");
        const localCommits = await parseLog(commits.length);
        const diffs = await extractDiff(localCommits);
        result.commitDiffs = diffs;
        console.log("   â†’ commitDiffs completed.");

        // 4ï¸âƒ£ ë¡œì»¬ git ë¡œê·¸ ì €ì¥
        console.log("\nğŸ“Œ Saving local git logs...");
        result.localLogs = localCommits;
        console.log(`   â†’ ${localCommits.length} logs saved.`);

        // 5ï¸âƒ£ JSON íŒŒì¼ë¡œ ì €ì¥ (Raw)
        fs.writeFileSync(
            path.join(outputDir, "pipeline_output.json"),
            JSON.stringify(result, null, 2),
            "utf-8"
        );

        // 6ï¸âƒ£ ë°ì´í„° ì •ì œ
        console.log("\nğŸ“Œ Data Refinement (NLP Preparation)...");
        refinedData = refineData(result);
        fs.writeFileSync(
            path.join(outputDir, "refined_data.json"),
            JSON.stringify(refinedData, null, 2),
            "utf-8"
        );
        console.log(`   â†’ ${refinedData.items.length} items refined.`);
    }

    // 7ï¸âƒ£ ì„ë² ë”© ìƒì„± ë° ì €ì¥ (OpenAI ë˜ëŠ” Chroma ê¸°ë³¸ ì„ë² ë”© fallback)
    console.log("\nğŸ“Œ Generating Embeddings...");
    try {
        const batchSize = 10;
        const items = refinedData.items;
        const embeddings: number[][] = [];

        // Batch processing to avoid huge payload
        for (let i = 0; i < items.length; i += batchSize) {
            const batch = items.slice(i, i + batchSize);
            const texts = batch.map((item: any) => item.content);
            console.log(`   Processing batch ${i / batchSize + 1}/${Math.ceil(items.length / batchSize)}...`);

            const batchEmbeddings = await generateEmbeddings(texts);
            embeddings.push(...batchEmbeddings);
        }

        console.log(`   â†’ Generated ${embeddings.length} vectors.`);

        console.log("\nğŸ“Œ Saving to ChromaDB...");
        // Collection name convention: repo-year-month or just repo-commits
        await saveVectors(`${repo}-commits`, items, embeddings, reset);

    } catch (err: any) {
        console.error("âŒ Embedding/Vector Store Failed:", err.message);
        console.error("   (Is ChromaDB running? 'pnpm run chroma:start')");
    }

    console.log("\nğŸ‰ Pipeline finished!");
    if (!skipFetch) {
        console.log("ğŸ“ Saved â†’ output/pipeline_output.json");
        console.log("ğŸ“ Saved â†’ output/refined_data.json");
    }
}

// ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
// ESM pattern to check if file is run directly
import { fileURLToPath } from "url";

if (process.argv[1] === fileURLToPath(import.meta.url)) {
    runPipeline().catch(err => {
        console.error("âŒ Pipeline failed:", err);
        process.exit(1);
    });
}
