# GitHub Repository Analyzer with NLP/RAG - 최종 발표 자료

> **시연 주소**: https://nlp-portfolio-zeta.vercel.app/  
> **프로젝트 개요**: GitHub 리포지토리 데이터를 RAG(Retrieval-Augmented Generation) 기반으로 분석하고 질문에 답변하는 시스템

---

## 목차

1. [프로젝트 소개](#프로젝트-소개)
2. [서비스의 이용성](#서비스의-이용성)
3. [시스템 아키텍처](#시스템-아키텍처)
4. [코드의 신뢰성](#코드의-신뢰성)
5. [유지보수성](#유지보수성)
6. [AI 모델의 실용성](#ai-모델의-실용성)
7. [시연](#시연)

---

## 프로젝트 소개

### 기술 스택

- **Backend**: Node.js + TypeScript + Vercel Serverless Functions (ESM)
- **Frontend**: React 19 + TypeScript + Vite + PandaCSS
- **State Management**: Jotai (atoms) + TanStack Query (server state)
- **Vector Storage**: File-based (Serverless) - GitHub Raw URL로 배포
- **LLM**: Claude Sonnet 4 (Primary) → OpenAI GPT-4o (Fallback 1) → Gemini 1.5 Flash (Fallback 2)
- **Embeddings**: Google Gemini text-embedding-004 (768 dimensions)
- **Storage**: Supabase (Q&A history, embedding pipeline)

### 핵심 아키텍처

```
사용자 질문
    ↓
임베딩 생성 (Gemini text-embedding-004)
    ↓
벡터 검색 (File-based, in-memory cosine similarity)
    ↓
Top-K 유사 문서 검색 (커밋 + 파일 + Q&A 히스토리)
    ↓
LLM 답변 생성 (Claude → OpenAI → Gemini Fallback)
    ↓
답변 + 출처 반환
```

### 비용 효율성

- **월 운영 비용: $0** (GitHub Free + Vercel Hobby)
- 벡터 저장소: GitHub Raw URL (무료 CDN)
- 서버리스 아키텍처: 사용량 기반 과금
- 데이터베이스: Supabase Free Tier (임베딩 파이프라인용)

---

## 서비스의 이용성

### 1. 온보딩 지원

#### 프로젝트 구조 및 현황 파악

새로운 팀원이나 온보딩이 필요한 개발자에게 프로젝트의 전체적인 구조와 현황을 빠르게 파악할 수 있도록 도와줍니다.

**지원하는 질문 유형:**

- **프로젝트 구조**: "프로젝트의 폴더 구조는 어떻게 되어있나요?"
- **기술 스택**: "이 프로젝트에서 사용하는 기술 스택은?"
- **아키텍처**: "시스템 아키텍처는 어떻게 설계되어 있나요?"
- **현황 파악**: "현재 프로젝트의 진행 상황은?"

**실제 구현 예시:**

```21:26:shared/services/qa/answer.ts
const context = sources
  .map((s, i) => {
    const typeLabel = s.type === 'commit' ? '커밋' : s.type === 'file' ? '파일' : 'Q&A';
    return `[${i + 1}] (${typeLabel}, 유사도: ${(s.score * 100).toFixed(1)}%)\n${s.content}`;
  })
  .join('\n\n---\n\n');
```

시스템은 커밋 메시지, 소스 코드 파일, 이전 Q&A 히스토리를 모두 검색하여 가장 관련성 높은 정보를 제공합니다.

#### 커밋 히스토리 기반 학습

**지원하는 질문 유형:**

- **변경 이력**: "이 기능은 언제 추가되었나요?"
- **개발 과정**: "이 버그는 어떻게 수정되었나요?"
- **의사결정**: "왜 이렇게 구현했는지 커밋 히스토리를 알려주세요"

**데이터 수집:**

```6:12:shared/models/EmbeddingItem.ts
export interface EmbeddingItem {
  id: string;
  type: 'commit' | 'file' | 'qa';
  content: string;
  embedding: number[];
  metadata: Record<string, any>;
}
```

시스템은 다음 세 가지 타입의 데이터를 수집합니다:
- **Commit**: 커밋 메시지와 변경된 파일 목록
- **File**: 실제 소스 코드 파일 (의미 단위로 청킹)
- **Q&A**: 이전 질문-답변 히스토리

### 2. 실제 소스 기반 응답으로 예측 가능성 보장

#### 소스 코드 기반 답변 생성

시스템은 **실제 구현된 소스 코드**를 기반으로 답변을 생성하므로, 특정 케이스에 대한 결과를 예측할 수 있습니다.

**구현 방식:**

```16:26:shared/services/qa/answer.ts
export async function generateAnswer(
  question: string,
  sources: SearchResult[]
): Promise<string> {
  // Build context from sources
  const context = sources
    .map((s, i) => {
      const typeLabel = s.type === 'commit' ? '커밋' : s.type === 'file' ? '파일' : 'Q&A';
      return `[${i + 1}] (${typeLabel}, 유사도: ${(s.score * 100).toFixed(1)}%)\n${s.content}`;
    })
    .join('\n\n---\n\n');
```

**프롬프트 설계:**

```28:36:shared/services/qa/answer.ts
const systemPrompt = `당신은 GitHub 리포지토리 분석 전문가입니다.
주어진 컨텍스트(커밋 메시지, 소스 코드, 이전 Q&A)를 기반으로 질문에 답변하세요.

답변 가이드라인:
- 한국어로 답변하세요
- 컨텍스트에서 관련 정보를 찾아 구체적으로 답변하세요
- 코드나 커밋 정보를 인용할 때는 출처를 명시하세요
- 컨텍스트에 관련 정보가 없으면 솔직히 "관련 정보를 찾지 못했습니다"라고 답변하세요
- 추측하지 말고 사실만 전달하세요`;
```

**핵심 특징:**

1. **출처 명시**: 각 답변에 사용된 소스의 유사도 점수와 타입 표시
2. **사실 기반**: 추측하지 않고 실제 코드/커밋만 인용
3. **예측 가능성**: 동일한 질문에 대해 동일한 소스 기반 답변 제공

#### 소스 코드 수집 전략

**파일 타입별 메타데이터:**

```35:43:shared/models/EmbeddingItem.ts
export interface FileMetadata {
  type: 'file';
  path: string;
  fileType: string;
  size: number;
  extension: string;
  chunkIndex?: number;
  totalChunks?: number;
}
```

**청킹 전략:**
- 큰 파일은 의미 단위로 분할 (클래스/메서드 경계 우선)
- 각 청크에 `chunkIndex`와 `totalChunks` 정보 포함
- 소스 코드의 실제 구조를 반영하여 검색 정확도 향상

**예시 질문과 예측 가능한 답변:**

| 질문 | 예측 가능성 |
|------|------------|
| "에러 핸들링은 어떻게 구현되어 있나요?" | ✅ 실제 `errorHandler.ts` 코드 기반 답변 |
| "벡터 검색은 어떤 알고리즘을 사용하나요?" | ✅ `fileVectorStore.ts`의 `cosineSimilarity` 함수 인용 |
| "LLM Fallback은 어떻게 동작하나요?" | ✅ `answer.ts`의 실제 Fallback 체인 코드 설명 |

**장점:**
- **신뢰성**: 실제 코드 기반이므로 정확한 정보 제공
- **일관성**: 동일한 질문에 대해 동일한 소스 인용
- **검증 가능**: 답변의 출처를 직접 확인 가능

---

## 시스템 아키텍처

### 전체 아키텍처 다이어그램

```mermaid
graph TB
    subgraph "Frontend (React + Vite)"
        UI[사용자 인터페이스]
        UI -->|질문 입력| API_CLIENT[API Client]
    end

    subgraph "Vercel Serverless Functions"
        API_CLIENT -->|POST /api/ask| ASK_ENDPOINT[ask.ts]
        ASK_ENDPOINT -->|질문 분류| CLASSIFIER[classifier.ts]
        ASK_ENDPOINT -->|임베딩 생성| EMBEDDING_SVC[embeddingService.ts]
        ASK_ENDPOINT -->|벡터 검색| VECTOR_STORE[fileVectorStore.ts]
        ASK_ENDPOINT -->|답변 생성| ANSWER_SVC[answer.ts]
    end

    subgraph "External APIs"
        EMBEDDING_SVC -->|text-embedding-004| GEMINI_API[Google Gemini API]
        ANSWER_SVC -->|Claude Sonnet 4| CLAUDE_API[Anthropic API]
        ANSWER_SVC -->|GPT-4o| OPENAI_API[OpenAI API]
        ANSWER_SVC -->|Gemini 1.5 Flash| GEMINI_API2[Google Gemini API]
    end

    subgraph "Vector Storage"
        VECTOR_STORE -->|로드| VECTOR_FILE[embeddings.json.gz]
        VECTOR_FILE -->|GitHub Raw URL| GITHUB_RAW[GitHub Raw CDN]
    end

    subgraph "Data Pipeline (Offline)"
        GITHUB_API[GitHub API] -->|커밋/파일 수집| PIPELINE[Unified Embedding Pipeline]
        PIPELINE -->|임베딩 생성| GEMINI_API3[Gemini API]
        PIPELINE -->|저장| SUPABASE[(Supabase pgvector)]
        SUPABASE -->|Export| VECTOR_FILE
    end

    subgraph "Storage"
        ASK_ENDPOINT -->|Q&A 히스토리 저장| SUPABASE
        SUPABASE -->|PostgreSQL| DB[(Database)]
    end

    style UI fill:#e1f5ff
    style ASK_ENDPOINT fill:#fff4e1
    style VECTOR_STORE fill:#e8f5e9
    style SUPABASE fill:#f3e5f5
    style GITHUB_RAW fill:#fff9c4
```

### 컴포넌트별 상세 설명

#### 1. Frontend (React + Vite)

**기술 스택:**
- React 19 + TypeScript
- Vite (빌드 도구)
- PandaCSS (스타일링)
- TanStack Query (서버 상태 관리)
- Jotai (클라이언트 상태 관리)

**주요 역할:**
- 사용자 질문 입력 인터페이스
- 답변 및 출처 표시
- Q&A 히스토리 관리
- 대시보드 통계 표시

#### 2. Vercel Serverless Functions

**구조:**
```
api/
├── ask.ts                    # Q&A 엔드포인트 (POST /api/ask)
├── health/                   # Health check
├── dashboard/                # 통계 대시보드
└── _lib/                     # 공유 유틸리티
    ├── cors.ts
    └── errorHandler.ts
```

**특징:**
- 파일 기반 라우팅 (Vercel 컨벤션)
- 서버리스 아키텍처 (사용량 기반 과금)
- 자동 스케일링
- Cold start 최적화 (벡터 파일 캐싱)

#### 3. Vector Storage (File-based)

**구조:**
```
output/
└── embeddings.json.gz        # 압축된 벡터 파일
    ├── version: "1.0"
    ├── statistics: {...}
    └── vectors: [
        {
          id: "commit-abc123",
          type: "commit",
          content: "...",
          embedding: [0.1, 0.2, ...],
          metadata: {...}
        },
        {
          id: "file-src/index.ts-0",
          type: "file",
          content: "...",
          embedding: [0.3, 0.4, ...],
          metadata: {...}
        }
    ]
```

**특징:**
- GitHub Raw URL로 호스팅 (무료 CDN)
- In-memory 캐싱 (5분 TTL)
- Gzip 압축 (약 10MB)
- 서버리스 환경에 최적화

#### 4. Embedding Pipeline (Offline)

**실행 주기:** 주간 (GitHub Actions)

**프로세스:**
```mermaid
sequenceDiagram
    participant GH as GitHub Actions
    participant GITHUB as GitHub API
    participant SUPABASE as Supabase
    participant GEMINI as Gemini API
    participant REPO as Git Repository

    GH->>GITHUB: 1. Fetch commits/files
    GITHUB-->>GH: Commit & file data
    
    GH->>SUPABASE: 2. Fetch Q&A history
    SUPABASE-->>GH: New Q&A items
    
    GH->>GEMINI: 3. Generate embeddings
    GEMINI-->>GH: 768-dim vectors
    
    GH->>SUPABASE: 4. Save to pgvector
    SUPABASE-->>GH: Success
    
    GH->>SUPABASE: 5. Export all embeddings
    SUPABASE-->>GH: All vectors
    
    GH->>GH: 6. Compress to .json.gz
    GH->>REPO: 7. Commit & push
```

**데이터 수집 범위:**
- 커밋 메시지 및 변경 파일
- 전체 소스 코드 파일 (의미 단위 청킹)
- Q&A 히스토리

#### 5. Supabase (PostgreSQL + pgvector)

**역할:**
- 임베딩 파이프라인 중간 저장소
- Q&A 히스토리 영구 저장
- 대시보드 통계 데이터

**테이블 구조:**
- `embeddings`: 벡터 데이터 (pgvector)
- `qa_history`: Q&A 기록
- `migrations`: 스키마 버전 관리

**특징:**
- Free Tier 사용 가능
- Q&A 쿼리 시에는 사용하지 않음 (파일 기반 사용)

### 데이터 흐름

#### Q&A 쿼리 플로우 (Online)

```mermaid
sequenceDiagram
    participant USER as 사용자
    participant FE as Frontend
    participant API as Vercel Function
    participant VS as Vector Store
    participant LLM as LLM API
    participant DB as Supabase

    USER->>FE: 질문 입력
    FE->>API: POST /api/ask
    
    API->>API: 1. 질문 분류 (로컬)
    API->>VS: 2. 임베딩 생성 요청
    VS->>VS: 3. Gemini API 호출
    VS-->>API: Query embedding
    
    API->>VS: 4. 벡터 검색
    VS->>VS: 5. 파일 로드 (캐시 확인)
    VS->>VS: 6. Cosine similarity 계산
    VS-->>API: Top-K 유사 문서
    
    API->>LLM: 7. 답변 생성 (Fallback 체인)
    LLM-->>API: 답변 텍스트
    
    API->>DB: 8. 히스토리 저장 (비동기)
    API-->>FE: 답변 + 출처
    FE-->>USER: 결과 표시
```

#### 임베딩 파이프라인 플로우 (Offline)

```mermaid
sequenceDiagram
    participant GHA as GitHub Actions
    participant GITHUB as GitHub API
    participant SUPABASE as Supabase
    participant GEMINI as Gemini API
    participant REPO as Git Repo

    Note over GHA: 주간 자동 실행
    
    GHA->>GITHUB: 1. Fetch commits (새로운 것만)
    GITHUB-->>GHA: Commit list
    
    GHA->>GITHUB: 2. Fetch repository files
    GITHUB-->>GHA: Source code
    
    GHA->>SUPABASE: 3. Fetch Q&A history (새로운 것만)
    SUPABASE-->>GHA: Q&A data
    
    loop For each item
        GHA->>GEMINI: 4. Generate embedding
        GEMINI-->>GHA: 768-dim vector
    end
    
    GHA->>SUPABASE: 5. Save embeddings
    SUPABASE-->>GHA: Success
    
    GHA->>SUPABASE: 6. Export all embeddings
    SUPABASE-->>GHA: All vectors
    
    GHA->>GHA: 7. Compress to .json.gz
    GHA->>REPO: 8. Commit & push
```

### 기술 스택 매핑

| 컴포넌트 | 기술 | 용도 |
|---------|------|------|
| **Frontend** | React 19 + Vite | 사용자 인터페이스 |
| **Serverless** | Vercel Functions | API 엔드포인트 |
| **Vector Storage** | GitHub Raw URL | 벡터 파일 호스팅 |
| **Embedding** | Gemini text-embedding-004 | 벡터 생성 (768차원) |
| **LLM** | Claude/OpenAI/Gemini | 답변 생성 |
| **Database** | Supabase (PostgreSQL) | Q&A 히스토리 저장 |
| **Pipeline** | GitHub Actions | 주간 자동 실행 |

---

## 코드의 신뢰성

### 1. Edge Case 처리

#### 입력 검증 및 빈 값 처리

```12:76:api/ask.ts
// Validate input
if (!question || typeof question !== 'string') {
  res.status(400).json({ error: 'Question is required' });
  return;
}

if (question.trim().length === 0) {
  res.status(400).json({ error: 'Question cannot be empty' });
  return;
}
```

**처리 사항:**
- `null`, `undefined`, 비문자열 타입 검증
- 빈 문자열 및 공백만 있는 입력 처리
- 적절한 HTTP 상태 코드 반환 (400 Bad Request)

#### 벡터 검색 결과 없음 처리

```99:126:api/ask.ts
if (sources.length === 0) {
  const processingTime = Date.now() - startTime;

  // Save failed attempt
  saveQAHistory(
    question,
    '관련 정보를 찾지 못했습니다.',
    [],
    {
      sessionId,
      category,
      categoryConfidence,
      status: 'partial',
      responseTimeMs: processingTime,
      classificationTimeMs: timings.classification,
      vectorSearchTimeMs: timings.vectorSearch,
    }
  ).catch((err) => console.error('[Q&A] Failed to save history:', err));

  const response: QAResponse = {
    answer: '관련 정보를 찾지 못했습니다. 다른 방식으로 질문해 주세요.',
    sources: [],
    processingTime,
    status: 'partial',
  };
  res.status(200).json(response);
  return;
}
```

**처리 사항:**
- 검색 결과가 없을 때 graceful degradation
- 부분 성공 상태(`partial`)로 명시적 표시
- 사용자에게 재질문 유도 메시지 제공

#### 벡터 차원 불일치 처리

```58:80:shared/services/vector-store/fileVectorStore.ts
function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    console.warn(`Vector dimension mismatch: ${a.length} vs ${b.length}`);
    return 0;
  }

  let dotProduct = 0;
  let normA = 0;
  let normB = 0;

  for (let i = 0; i < a.length; i++) {
    const aVal = a[i] ?? 0;
    const bVal = b[i] ?? 0;
    dotProduct += aVal * bVal;
    normA += aVal * aVal;
    normB += bVal * bVal;
  }

  const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
  if (magnitude === 0) return 0;

  return dotProduct / magnitude;
}
```

**처리 사항:**
- 벡터 차원 불일치 시 경고 로그 및 안전한 반환값(0)
- `undefined` 값 처리 (`??` 연산자 사용)
- 0으로 나누기 방지 (magnitude 체크)

#### Token 카운팅 Fallback

```29:37:api/ask.ts
function countTokens(text: string): number {
  try {
    const encoder = getTokenEncoder();
    return encoder.encode(text).length;
  } catch {
    // Fallback: rough estimate (1 token ≈ 4 characters for English, 2 for Korean)
    return Math.ceil(text.length / 3);
  }
}
```

**처리 사항:**
- tiktoken 라이브러리 실패 시 대략적 추정값 반환
- 시스템이 완전히 중단되지 않도록 보장

### 2. Fallback 메커니즘

#### LLM API Fallback 체인

```46:77:shared/services/qa/answer.ts
// Try Claude first
if (process.env.CLAUDE_API_KEY) {
  try {
    const answer = await generateWithClaude(systemPrompt, userPrompt);
    if (answer) return answer;
  } catch (error) {
    console.error('[LLM] Claude API failed:', error);
  }
}

// Try OpenAI
if (process.env.OPENAI_API_KEY) {
  try {
    const answer = await generateWithOpenAI(systemPrompt, userPrompt);
    if (answer) return answer;
  } catch (error) {
    console.error('[LLM] OpenAI API failed:', error);
  }
}

// Try Gemini
if (process.env.GEMINI_API_KEY) {
  try {
    const answer = await generateWithGemini(systemPrompt, userPrompt);
    if (answer) return answer;
  } catch (error) {
    console.error('[LLM] Gemini API failed:', error);
  }
}

return '죄송합니다. 현재 답변을 생성할 수 없습니다. API 키를 확인해주세요.';
```

**Fallback 전략:**
1. **Primary**: Claude Sonnet 4 (최고 품질)
2. **Fallback 1**: OpenAI GPT-4o (안정성)
3. **Fallback 2**: Gemini 1.5 Flash (비용 효율)
4. **Final Fallback**: 사용자 친화적 에러 메시지

**장점:**
- 단일 API 장애 시에도 서비스 지속 가능
- API 키 누락/만료 시 명확한 에러 메시지
- 각 API 실패를 독립적으로 처리 (다른 API에 영향 없음)

#### 에러 핸들링 및 상태 코드 매핑

```8:34:api/_lib/errorHandler.ts
export function handleError(
  res: VercelResponse,
  error: unknown,
  message: string
): void {
  console.error(`[ERROR] ${message}:`, error);

  const errorMessage = error instanceof Error ? error.message : 'Unknown error';
  const statusCode = getStatusCode(error);

  res.status(statusCode).json({
    error: message,
    details: errorMessage,
    timestamp: new Date().toISOString(),
  });
}

function getStatusCode(error: unknown): number {
  if (error instanceof Error) {
    const message = error.message.toLowerCase();
    if (message.includes('not found')) return 404;
    if (message.includes('unauthorized') || message.includes('api key')) return 401;
    if (message.includes('invalid') || message.includes('required')) return 400;
    if (message.includes('rate limit')) return 429;
  }
  return 500;
}
```

**처리 사항:**
- 에러 타입별 적절한 HTTP 상태 코드 반환
- 타임스탬프 포함으로 디버깅 용이
- `unknown` 타입 안전 처리

#### Q&A 히스토리 저장 실패 처리

```143:171:api/ask.ts
// Step 5: Save to Q&A history (await to ensure completion before response)
console.log('[Q&A] Step 5: Saving to history...');
const dbSaveStart = Date.now();
try {
  await saveQAHistory(
    question,
    answer,
    sources.map((s) => ({ id: s.id, type: s.type, score: s.score })),
    {
      sessionId,
      questionSummary: question.slice(0, 20),
      category,
      categoryConfidence,
      status: 'success',
      responseTimeMs: processingTime,
      classificationTimeMs: timings.classification,
      vectorSearchTimeMs: timings.vectorSearch,
      llmGenerationTimeMs: timings.llmGeneration,
      llmProvider: getLLMProvider(),
      tokenUsage: totalTokens,
      promptTokens,
      completionTokens,
      embeddingTokens,
    }
  );
  timings.dbSave = Date.now() - dbSaveStart;
} catch (err) {
  console.error('[Q&A] Failed to save history:', err);
}
```

**처리 사항:**
- DB 저장 실패 시에도 사용자 응답은 정상 반환
- 에러 로깅으로 모니터링 가능
- 비동기 작업 실패가 메인 플로우에 영향 없음

#### 프론트엔드 Silent Mode

```29:64:src/api/client.ts
async function request<T>(
  endpoint: string,
  options?: RequestInit,
  silent = false
): Promise<T> {
  const url = `${API_BASE_URL}${endpoint}`;

  try {
    const response = await fetch(url, {
      ...options,
      headers: {
        'Content-Type': 'application/json',
        ...options?.headers,
      },
    });

    if (!response.ok) {
      if (silent) {
        return null as T;
      }
      const error = await response.json().catch(() => ({}));
      throw new ApiError(
        error.error || `HTTP ${response.status}`,
        response.status,
        error.details
      );
    }

    return response.json();
  } catch (error) {
    if (silent) {
      return null as T;
    }
    throw error;
  }
}
```

**처리 사항:**
- Health check 등 비중요 API는 `silent=true`로 설정
- 실패 시 `null` 반환으로 UI 크래시 방지
- 사용자 경험에 영향 없는 에러는 조용히 처리

---

## 유지보수성

### 1. 모듈화

#### 계층별 명확한 분리

```
api/                          # Vercel Serverless Functions (라우팅)
├── ask.ts                    # Q&A 엔드포인트
├── health/                   # Health check 엔드포인트
└── _lib/                     # 공유 유틸리티
    ├── cors.ts               # CORS 설정
    └── errorHandler.ts       # 에러 처리

shared/                       # 비즈니스 로직 (재사용 가능)
├── services/
│   ├── qa/
│   │   ├── answer.ts         # LLM 답변 생성
│   │   └── classifier.ts     # 질문 분류
│   └── vector-store/
│       ├── fileVectorStore.ts    # 벡터 검색
│       └── embeddingService.ts   # 임베딩 생성
└── models/                   # 타입 정의
    ├── EmbeddingItem.ts
    └── SearchResult.ts

src/                          # Frontend
├── api/
│   └── client.ts            # API 클라이언트
├── components/              # React 컴포넌트
└── pages/                   # 페이지 컴포넌트
```

**모듈화 원칙:**
- **관심사 분리**: 라우팅, 비즈니스 로직, UI 분리
- **의존성 방향**: `api/` → `shared/` → `src/` (단방향)
- **재사용성**: `shared/` 모듈은 독립적으로 테스트 가능

#### 독립적인 서비스 모듈

각 서비스는 단일 책임 원칙을 따릅니다:

```1:41:shared/services/vector-store/embeddingService.ts
/**
 * Embedding Service
 * 쿼리 임베딩 생성 (Gemini API - 768차원)
 *
 * 주의: Supabase에 저장된 임베딩과 동일한 모델(text-embedding-004)을 사용해야 합니다.
 */

import { GoogleGenerativeAI } from '@google/generative-ai';

let genAI: GoogleGenerativeAI | null = null;

function getGenAI(): GoogleGenerativeAI {
  if (genAI) return genAI;

  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    throw new Error('Missing GEMINI_API_KEY environment variable');
  }

  genAI = new GoogleGenerativeAI(apiKey);
  return genAI;
}

/**
 * 쿼리 텍스트에 대한 임베딩 생성
 * @param query - 사용자 질문
 * @returns 768차원 벡터
 */
export async function generateQueryEmbedding(query: string): Promise<number[]> {
  const model = getGenAI().getGenerativeModel({ model: 'text-embedding-004' });

  try {
    const result = await model.embedContent(query);
    return result.embedding.values;
  } catch (error) {
    console.error('Failed to generate query embedding:', error);
    throw new Error(
      `Embedding generation failed: ${error instanceof Error ? error.message : 'Unknown error'}`
    );
  }
}
```

**모듈 독립성:**
- 단일 함수 export (`generateQueryEmbedding`)
- 내부 구현 변경 시 외부 영향 없음
- Singleton 패턴으로 API 클라이언트 재사용

### 2. 주석 및 문서화

#### JSDoc 스타일 주석

모든 주요 함수와 모듈에 JSDoc 주석이 포함되어 있습니다:

```1:7:api/ask.ts
/**
 * Q&A Endpoint
 * POST /api/ask
 *
 * 질문을 받아 벡터 검색 후 LLM으로 답변 생성
 */
```

```1:6:shared/services/qa/answer.ts
/**
 * Answer Generation Service
 * LLM을 사용한 답변 생성 (Fallback 체인)
 *
 * 우선순위: Claude → OpenAI → Gemini
 */
```

```1:8:shared/services/vector-store/fileVectorStore.ts
/**
 * File-based Vector Store
 * 파일 기반 벡터 검색 (in-memory cosine similarity)
 *
 * 벡터 파일: output/embeddings.json.gz
 * 캐싱: 5분 TTL
 */
```

**주석 가이드라인:**
- 파일 상단: 모듈 목적과 주요 기능 설명
- 함수: 파라미터, 반환값, 예외 상황 설명
- 복잡한 로직: 인라인 주석으로 의도 명시

#### 타입 정의 및 인터페이스

```6:24:shared/services/qa/classifier.ts
export type QuestionCategory =
  | 'planning'
  | 'technical'
  | 'history'
  | 'cs'
  | 'status'
  | 'issue'
  | 'implementation'
  | 'structure'
  | 'data'
  | 'techStack'
  | 'testing'
  | 'summary'
  | 'etc';

export interface ClassificationResult {
  category: QuestionCategory;
  confidence: number;
}
```

**타입 안정성:**
- TypeScript strict mode 활성화
- 모든 API 응답에 타입 정의
- `unknown` 타입으로 안전한 에러 처리

### 3. 형상 관리

#### Git 커밋 컨벤션

프로젝트는 일관된 커밋 메시지 컨벤션을 따릅니다:

```
<type>: <한국어로 작성된 커밋 메시지>
```

**예시:**
- `feat: 새로운 임베딩 export 스크립트 추가`
- `fix: Supabase 연결 오류 수정`
- `refactor: 벡터 검색 로직 개선`
- `docs: README에 설치 가이드 추가`

**장점:**
- 커밋 히스토리 가독성 향상
- 자동화된 릴리즈 노트 생성 가능
- 팀 협업 시 변경 사항 파악 용이

#### CI/CD 파이프라인

**자동 배포:**
- GitHub `main` 브랜치 푸시 시 Vercel 자동 배포
- 빌드 실패 시 이전 버전 유지
- 환경 변수는 Vercel 대시보드에서 관리

**임베딩 파이프라인:**
- 주간 GitHub Actions로 자동 실행
- 새로운 커밋/파일 감지 시 자동 임베딩 생성
- `output/embeddings.json.gz` 자동 업데이트 및 커밋

#### 버전 관리

```1:5:package.json
{
  "name": "nlp-portfolio",
  "version": "1.0.0",
  "type": "module",
  "description": "GitHub Repository Analyzer with NLP/RAG",
```

- 시맨틱 버저닝 준수
- `package.json`에 명확한 버전 정보
- 의존성 버전 고정 (`pnpm-lock.yaml`)

---

## AI 모델의 실용성

### 1. Latency (응답 시간)

#### 성능 측정 및 로깅

```55:62:api/ask.ts
const startTime = Date.now();
const timings = {
  classification: 0,
  embedding: 0,
  vectorSearch: 0,
  llmGeneration: 0,
  dbSave: 0,
};
```

각 단계별 소요 시간을 측정하여 병목 지점을 파악합니다:

```80:97:api/ask.ts
// Step 1: Classify question
console.log('[Q&A] Step 1: Classifying question...');
const classificationStart = Date.now();
const { category, confidence: categoryConfidence } = classifyQuestion(question);
timings.classification = Date.now() - classificationStart;
console.log(`[Q&A] Category: ${category} (confidence: ${categoryConfidence})`);

// Step 2: Generate query embedding
console.log('[Q&A] Step 2: Generating query embedding...');
const embeddingStart = Date.now();
const queryEmbedding = await generateQueryEmbedding(question);
timings.embedding = Date.now() - embeddingStart;

// Step 3: Search similar vectors
console.log('[Q&A] Step 3: Searching similar vectors...');
const vectorSearchStart = Date.now();
const sources = await searchSimilar(queryEmbedding, topK);
timings.vectorSearch = Date.now() - vectorSearchStart;
```

**최적화 전략:**

1. **벡터 파일 캐싱**
```15:29:shared/services/vector-store/fileVectorStore.ts
// Cache for vector file
let cachedVectors: VectorFile | null = null;
let cacheExpiry: number = 0;
const CACHE_TTL = 5 * 60 * 1000; // 5분

/**
 * 벡터 파일 로드 (캐싱 적용)
 */
async function loadVectorFile(): Promise<VectorFile> {
  const now = Date.now();

  // Return cached data if still valid
  if (cachedVectors && now < cacheExpiry) {
    return cachedVectors;
  }
```

- **Cold start**: 150-380ms (파일 로드)
- **Warm start**: 51-151ms (캐시 히트)
- 5분 TTL로 메모리 사용량과 성능 균형

2. **In-memory 벡터 검색**
- 파일 기반 저장소로 네트워크 지연 없음
- Brute-force cosine similarity (작은 데이터셋에 최적)
- 병렬 처리 가능한 구조

3. **비동기 처리**
- DB 저장은 응답 후 비동기 처리 (필요 시)
- 사용자 응답 지연 최소화

**예상 응답 시간:**
- 질문 분류: < 10ms (로컬 처리)
- 임베딩 생성: 200-500ms (Gemini API)
- 벡터 검색: 50-150ms (in-memory)
- LLM 답변 생성: 1-3초 (Claude/OpenAI)
- **총 응답 시간: 1.5-4초** (사용자 대기 가능 범위)

### 2. Prompt Engineering

#### 구조화된 프롬프트

```28:44:shared/services/qa/answer.ts
const systemPrompt = `당신은 GitHub 리포지토리 분석 전문가입니다.
주어진 컨텍스트(커밋 메시지, 소스 코드, 이전 Q&A)를 기반으로 질문에 답변하세요.

답변 가이드라인:
- 한국어로 답변하세요
- 컨텍스트에서 관련 정보를 찾아 구체적으로 답변하세요
- 코드나 커밋 정보를 인용할 때는 출처를 명시하세요
- 컨텍스트에 관련 정보가 없으면 솔직히 "관련 정보를 찾지 못했습니다"라고 답변하세요
- 추측하지 말고 사실만 전달하세요`;

const userPrompt = `## 컨텍스트
${context}

## 질문
${question}

## 답변`;
```

**프롬프트 설계 원칙:**

1. **역할 명확화**: "GitHub 리포지토리 분석 전문가"
2. **구체적 가이드라인**: 한국어, 출처 명시, 사실만 전달
3. **구조화된 입력**: 컨텍스트와 질문을 명확히 구분
4. **유사도 정보 포함**: 각 소스의 유사도 점수 제공

```21:26:shared/services/qa/answer.ts
const context = sources
  .map((s, i) => {
    const typeLabel = s.type === 'commit' ? '커밋' : s.type === 'file' ? '파일' : 'Q&A';
    return `[${i + 1}] (${typeLabel}, 유사도: ${(s.score * 100).toFixed(1)}%)\n${s.content}`;
  })
  .join('\n\n---\n\n');
```

**일관성 보장:**
- System prompt로 답변 스타일 고정
- 컨텍스트 형식 일관성 유지
- 유사도 점수로 신뢰도 전달

### 3. Cost Efficiency

#### 비용 최적화 전략

**1. 파일 기반 벡터 저장소 (무료)**

```1:8:shared/services/vector-store/fileVectorStore.ts
/**
 * File-based Vector Store
 * 파일 기반 벡터 검색 (in-memory cosine similarity)
 *
 * 벡터 파일: output/embeddings.json.gz
 * 캐싱: 5분 TTL
 */
```

- GitHub Raw URL로 호스팅 (무료 CDN)
- Supabase pgvector 대신 사용 (월 $25-30 절감)
- 서버리스 환경에 최적화

**2. LLM Fallback 체인 (비용 효율적 우선순위)**

```46:77:shared/services/qa/answer.ts
// Try Claude first (최고 품질, 비용 높음)
// Try OpenAI (균형잡힌 품질/비용)
// Try Gemini (비용 효율, 품질 양호)
```

**비용 비교 (1,000 질문 기준):**
- Claude Sonnet 4: ~$15-20
- OpenAI GPT-4o: ~$10-15
- Gemini 1.5 Flash: ~$0.5-1

**3. Token 사용량 추적**

```136:141:api/ask.ts
// Calculate token usage
const contextText = sources.map((s) => s.content).join('\n');
const promptTokens = countTokens(question + contextText);
const completionTokens = countTokens(answer);
const embeddingTokens = countTokens(question);
const totalTokens = promptTokens + completionTokens;
```

- 각 요청별 토큰 사용량 기록
- 대시보드에서 비용 모니터링 가능
- 비용 예측 및 최적화 기반 데이터 제공

**4. Top-K 검색 최적화**

```88:107:shared/services/vector-store/fileVectorStore.ts
export async function searchSimilar(
  queryEmbedding: number[],
  topK: number = 5
): Promise<SearchResult[]> {
  const vectorFile = await loadVectorFile();

  // Calculate similarity for all vectors
  const scored: SearchResult[] = vectorFile.vectors.map((item: EmbeddingItem) => ({
    id: item.id,
    type: item.type,
    content: item.content,
    metadata: item.metadata,
    score: cosineSimilarity(queryEmbedding, item.embedding),
  }));

  // Sort by score (descending) and take top K
  scored.sort((a, b) => b.score - a.score);

  return scored.slice(0, topK);
}
```

- 기본 `topK=5`로 컨텍스트 크기 제한
- 프롬프트 토큰 수 감소로 비용 절감
- 필요 시 동적 조정 가능

**5. 데이터 정리 정책**

- 6개월 이상 된 데이터 자동 삭제
- 삭제된 파일 임베딩 자동 제거
- 10MB 용량 제한으로 파일 크기 관리

**월 예상 비용 (1,000 질문/월 기준):**
- Vercel: $0 (Hobby 플랜)
- GitHub: $0 (Free 플랜)
- Supabase: $0 (Free Tier, Q&A 히스토리만 저장)
- LLM API: $5-15 (사용량에 따라)
- **총 비용: $5-15/월** (사용자 수 증가 시에도 선형 확장)

---

## 시연

### 시연 1: Q&A 기능

**시나리오**: 프로젝트의 기술 스택에 대한 질문

1. **질문 입력**: "프로젝트의 기술 스택은?"
2. **처리 과정**:
   - 질문 분류: `techStack` 카테고리
   - 임베딩 생성: Gemini API 호출
   - 벡터 검색: 유사한 문서 5개 검색
   - LLM 답변: Claude/OpenAI로 답변 생성
3. **결과**:
   - 답변: 기술 스택 상세 설명
   - 출처: 관련 커밋/파일 5개 표시
   - 처리 시간: 1.5-4초

**시연 주소**: https://nlp-portfolio-zeta.vercel.app/

### 시연 2: Fallback 메커니즘

**시나리오**: Primary LLM API 장애 상황

1. **Primary API (Claude) 실패 시**:
   - 자동으로 OpenAI로 전환
   - 사용자는 중단 없이 답변 수신
2. **모든 API 실패 시**:
   - 명확한 에러 메시지 표시
   - 시스템 크래시 없음

**코드 위치**: `shared/services/qa/answer.ts:46-77`

---

## 결론

### 강점 요약

1. **신뢰성**: 포괄적인 Edge case 처리 및 Fallback 메커니즘
2. **유지보수성**: 명확한 모듈화, 상세한 주석, 일관된 Git 컨벤션
3. **실용성**: 사용자 대기 가능한 응답 시간, 일관된 답변 품질, 낮은 운영 비용

### 개선 가능 영역

1. **성능**: 벡터 검색 알고리즘 최적화 (FAISS 등)
2. **모니터링**: APM 도구 통합 (Sentry, Datadog)
3. **테스트**: 단위 테스트 및 통합 테스트 추가

---

**프로젝트 저장소**: [GitHub Repository](https://github.com/your-username/nlp-portfolio)  
**시연 주소**: https://nlp-portfolio-zeta.vercel.app/
