# Serverless API Design (Vercel)

> ì‚¬ì „ ìƒì„±ëœ embeddings JSONì„ ì‚¬ìš©í•œ ì½ê¸° ì „ìš© ì§ˆì˜ì‘ë‹µ API
>
> **ëª©í‘œ**: ì„œë²„ ë¹„ìš© $0, ë¹ ë¥¸ ì‘ë‹µ, ìƒíƒœ ë¹„ì €ì¥ (stateless)

---

## ğŸ“‹ ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ê°œìš”](#ì•„í‚¤í…ì²˜-ê°œìš”)
2. [/api/ask ë‚´ë¶€ ì²˜ë¦¬ íë¦„](#apiask-ë‚´ë¶€-ì²˜ë¦¬-íë¦„)
3. [ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸](#ì„±ëŠ¥-ìµœì í™”-í¬ì¸íŠ¸)
4. [Serverless ì œí•œì‚¬í•­ ê³ ë ¤](#serverless-ì œí•œì‚¬í•­-ê³ ë ¤)
5. [ë°°í¬ ë° ì„¤ì •](#ë°°í¬-ë°-ì„¤ì •)
6. [ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”](#ëª¨ë‹ˆí„°ë§-ë°-ìµœì í™”)

---

## ì•„í‚¤í…ì²˜ ê°œìš”

### Serverless vs Traditional Server

```
Traditional Server (Express)              Serverless (Vercel Functions)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  24/7 Running       â”‚                  â”‚  On-Demand          â”‚
â”‚  $20-50/ì›”          â”‚                  â”‚  $0/ì›” (ë¬´ë£Œ tier)   â”‚
â”‚  Always Warm        â”‚                  â”‚  Cold Start ë°œìƒ     â”‚
â”‚  Stateful ê°€ëŠ¥      â”‚                  â”‚  Statelessë§Œ ê°€ëŠ¥    â”‚
â”‚  ChromaDB ì—°ê²°      â”‚                  â”‚  File-basedë§Œ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì‹œìŠ¤í…œ êµ¬ì„±

```
User Request â†’ Vercel Edge Network â†’ Serverless Function
                                           â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ 1. Load JSON â”‚ (CDN/Blob)
                                    â”‚ 2. Embed Q   â”‚ (OpenAI)
                                    â”‚ 3. Search    â”‚ (In-Memory)
                                    â”‚ 4. Generate  â”‚ (OpenAI/Claude)
                                    â”‚ 5. Save Log  â”‚ (Supabase)
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â†“
                                    JSON Response
```

**í•µì‹¬ ì„¤ê³„ ì›ì¹™**:
1. **ì½ê¸° ì „ìš©**: ì„ë² ë”© ë°ì´í„°ëŠ” CIì—ì„œ ìƒì„±, APIëŠ” ì½ê¸°ë§Œ
2. **íŒŒì¼ ê¸°ë°˜**: ChromaDB ì„œë²„ ë¶ˆí•„ìš”, ì •ì  JSON íŒŒì¼ ì‚¬ìš©
3. **ë©”ëª¨ë¦¬ ìºì‹±**: Lambda/Vercel ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš©ìœ¼ë¡œ Warm Start
4. **ë¹„ë™ê¸° ë¡œê¹…**: Supabase ì €ì¥ ì‹¤íŒ¨í•´ë„ ì‘ë‹µ ë°˜í™˜

---

## /api/ask ë‚´ë¶€ ì²˜ë¦¬ íë¦„

### Phase 1: ìš”ì²­ ê²€ì¦ ë° ì´ˆê¸°í™” (0-5ms)

```typescript
// api/ask.ts
export default async function handler(req: VercelRequest, res: VercelResponse) {
  const startTime = Date.now();

  // 1-1. CORS ì„¤ì • (Vercel Edgeì—ì„œ ì²˜ë¦¬)
  res.setHeader('Access-Control-Allow-Origin', '*');

  // 1-2. HTTP Method í™•ì¸
  if (req.method === 'OPTIONS') {
    res.status(200).end();
    return;
  }

  if (req.method !== 'POST') {
    res.status(405).json({ error: 'Method not allowed' });
    return;
  }

  // 1-3. ìš”ì²­ íŒŒì‹±
  const { question, sessionId } = req.body;

  // 1-4. ì…ë ¥ ê²€ì¦
  if (!question || typeof question !== 'string') {
    res.status(400).json({ error: 'ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.' });
    return;
  }

  // 1-5. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (Serverlessì—ì„œëŠ” VECTOR_FILE_URLë§Œ ì‚¬ìš©)
  const vectorFileUrl = process.env.VECTOR_FILE_URL;
  if (!vectorFileUrl) {
    res.status(500).json({ error: 'Vector file URL not configured' });
    return;
  }
}
```

**ì²´í¬í¬ì¸íŠ¸ 1**:
- âœ… ì…ë ¥ ìœ íš¨ì„± ê²€ì¦
- âœ… í™˜ê²½ ì„¤ì • í™•ì¸
- â±ï¸ í‰ê·  ì†Œìš” ì‹œê°„: 0-5ms

---

### Phase 2: ì§ˆë¬¸ ë¶„ë¥˜ (5-10ms)

```typescript
// 2. ì§ˆë¬¸ ë¶„ë¥˜ (Rule-based, LLM í˜¸ì¶œ ì—†ìŒ)
const { category, confidence } = classifyQuestionWithConfidence(question);
```

**ë¶„ë¥˜ ë¡œì§**:
```typescript
// src/service/qa/classifier.ts (ì˜ˆì‹œ)
export function classifyQuestionWithConfidence(question: string): {
  category: string;
  confidence: number;
} {
  const lowerQ = question.toLowerCase();

  // ê¸°ìˆ  ìŠ¤íƒ ì§ˆë¬¸
  if (lowerQ.match(/ê¸°ìˆ |ìŠ¤íƒ|ë¼ì´ë¸ŒëŸ¬ë¦¬|í”„ë ˆì„ì›Œí¬|ì‚¬ìš©/)) {
    return { category: 'tech_stack', confidence: 0.9 };
  }

  // êµ¬í˜„ ì§ˆë¬¸
  if (lowerQ.match(/ì–´ë–»ê²Œ|êµ¬í˜„|ë°©ë²•|ì½”ë“œ|ë¡œì§/)) {
    return { category: 'implementation', confidence: 0.85 };
  }

  // íˆìŠ¤í† ë¦¬ ì§ˆë¬¸
  if (lowerQ.match(/ì–¸ì œ|ì»¤ë°‹|ë³€ê²½|ìˆ˜ì •|ì¶”ê°€/)) {
    return { category: 'history', confidence: 0.8 };
  }

  // ì¼ë°˜ ì§ˆë¬¸
  return { category: 'general', confidence: 0.5 };
}
```

**ì²´í¬í¬ì¸íŠ¸ 2**:
- âœ… ë¹ ë¥¸ ë¶„ë¥˜ (ì •ê·œì‹ ê¸°ë°˜)
- â±ï¸ í‰ê·  ì†Œìš” ì‹œê°„: 5-10ms

---

### Phase 3: ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (100-300ms)

```typescript
// 3-1. OpenAI API í˜¸ì¶œ (ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬)
const queryEmbedding = await generateQueryEmbedding(question);
```

**ì„ë² ë”© ì„œë¹„ìŠ¤**:
```typescript
// src/service/vector-store/embeddingService.ts
export async function generateQueryEmbedding(query: string): Promise<number[]> {
  const embeddings = await generateEmbeddings([query]);

  if (!embeddings || embeddings.length === 0) {
    throw new Error("Failed to generate query embedding");
  }

  return embeddings[0]; // 1536ì°¨ì› ë²¡í„°
}
```

**OpenAI API í˜¸ì¶œ**:
```typescript
// src/embedding-pipeline/nlp/embedding/openaiEmbedding.ts
const response = await openai.embeddings.create({
  model: "text-embedding-3-small",
  input: [query]
});

return response.data.map(d => d.embedding);
```

**ì²´í¬í¬ì¸íŠ¸ 3**:
- âœ… OpenAI API í˜¸ì¶œ ì„±ê³µ
- â±ï¸ í‰ê·  ì†Œìš” ì‹œê°„: 100-300ms (ë„¤íŠ¸ì›Œí¬ ì§€ì—°)
- âš ï¸ ìµœëŒ€ ë³‘ëª© êµ¬ê°„ (ì™¸ë¶€ API ì˜ì¡´)

---

### Phase 4: ë²¡í„° ê²€ìƒ‰ (Cold: 150-380ms, Warm: 51-151ms)

```typescript
// 4. íŒŒì¼ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ (ë©”ëª¨ë¦¬ ìºì‹±)
const contexts = await searchVectorsFromFile(queryEmbedding, 5, {
  threshold: 0.0,
  filterMetadata: { owner, repo }
});
```

**ë²¡í„° íŒŒì¼ ë¡œë”© (ë©”ëª¨ë¦¬ ìºì‹±)**:
```typescript
// src/service/vector-store/fileVectorStore.ts

// ë©”ëª¨ë¦¬ ìºì‹œ (Lambda ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš©)
let cachedIndex: VectorIndex | null = null;
let cacheTimestamp: number = 0;
const CACHE_TTL_MS = 5 * 60 * 1000; // 5ë¶„

async function loadVectorIndex(): Promise<VectorIndex> {
  const now = Date.now();

  // âœ… Warm Start: ìºì‹œ íˆíŠ¸
  if (cachedIndex && (now - cacheTimestamp) < CACHE_TTL_MS) {
    console.log("âœ… Using cached vector index");
    return cachedIndex;
  }

  // â„ï¸ Cold Start: íŒŒì¼ ë‹¤ìš´ë¡œë“œ
  console.log("ğŸ“¥ Loading vector index from file...");
  const startTime = Date.now();

  const vectorFileUrl = process.env.VECTOR_FILE_URL;

  const response = await fetch(vectorFileUrl, {
    headers: { 'Accept-Encoding': 'gzip' }
  });

  const buffer = Buffer.from(await response.arrayBuffer());

  // Gzip ì••ì¶• í•´ì œ
  let jsonString: string;
  if (vectorFileUrl.endsWith('.gz')) {
    const decompressed = await gunzipAsync(buffer);
    jsonString = decompressed.toString('utf-8');
  } else {
    jsonString = buffer.toString('utf-8');
  }

  const index: VectorIndex = JSON.parse(jsonString);

  // ìºì‹œ ì—…ë°ì´íŠ¸
  cachedIndex = index;
  cacheTimestamp = now;

  const loadTime = Date.now() - startTime;
  console.log(`âœ… Loaded ${index.count} vectors in ${loadTime}ms`);

  return index;
}
```

**ë¸Œë£¨íŠ¸í¬ìŠ¤ ê²€ìƒ‰ (ë©”ëª¨ë¦¬)**:
```typescript
export async function searchVectorsFromFile(
  queryEmbedding: number[],
  topK: number = 5,
  options?: { threshold?: number; filterMetadata?: Record<string, any> }
): Promise<SearchResult[]> {
  const { threshold = 0.0, filterMetadata } = options || {};

  // 1. ë²¡í„° íŒŒì¼ ë¡œë”© (ìºì‹œ ìš°ì„ )
  const index = await loadVectorIndex();

  // 2. ë©”íƒ€ë°ì´í„° í•„í„°ë§ + ìœ ì‚¬ë„ ê³„ì‚°
  const similarities: Array<{ id: string; score: number; data: VectorData }> = [];

  for (const vec of index.vectors) {
    // ë©”íƒ€ë°ì´í„° í•„í„°
    if (filterMetadata) {
      const matches = Object.entries(filterMetadata).every(
        ([key, value]) => vec.metadata[key] === value
      );
      if (!matches) continue;
    }

    // ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ìˆœìˆ˜ JavaScript)
    const score = cosineSimilarity(queryEmbedding, vec.embedding);

    if (score >= threshold) {
      similarities.push({ id: vec.id, score, data: vec });
    }
  }

  // 3. Top-K ì¶”ì¶œ (ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
  similarities.sort((a, b) => b.score - a.score);
  const topResults = similarities.slice(0, topK);

  // 4. SearchResult ë³€í™˜
  return topResults.map(result => ({
    id: result.data.id,
    content: result.data.content,
    metadata: result.data.metadata,
    score: result.score
  }));
}
```

**ì²´í¬í¬ì¸íŠ¸ 4**:
- âœ… Cold Start (ì²« ìš”ì²­): íŒŒì¼ ë‹¤ìš´ë¡œë“œ + JSON íŒŒì‹± (150-380ms)
- âœ… Warm Start (ìºì‹œ íˆíŠ¸): ë©”ëª¨ë¦¬ ê²€ìƒ‰ë§Œ (51-151ms)
- â±ï¸ í‰ê·  ì†Œìš” ì‹œê°„: 100-200ms

---

### Phase 5: LLM ë‹µë³€ ìƒì„± (1000-3000ms)

```typescript
// 5. OpenAI/Claudeë¡œ ë‹µë³€ ìƒì„±
const { answer, usage } = await generateAnswerWithUsage(question, contexts);
```

**ë‹µë³€ ìƒì„± ë¡œì§**:
```typescript
// src/service/qa/answer.ts
export async function generateAnswerWithUsage(
  query: string,
  results: SearchResult[]
): Promise<{ answer: string; usage: TokenUsage }> {
  // Context ë¬¸ìì—´ ìƒì„±
  const contextText = buildContext(results);

  if (!contextText) {
    return {
      answer: "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
      usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 }
    };
  }

  try {
    // OpenAI ìš°ì„  ì‹œë„
    if (openai) {
      return await generateWithOpenAIAndUsage(query, contextText);
    }

    // Claude fallback
    if (anthropic) {
      return await generateWithClaudeAndUsage(query, contextText);
    }

    // ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì—ëŸ¬
    throw new Error("No LLM API configured");

  } catch (error: any) {
    console.error("âŒ LLM ìƒì„± ì˜¤ë¥˜:", error.message);
    return {
      answer: "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
      usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 }
    };
  }
}
```

**OpenAI í˜¸ì¶œ**:
```typescript
async function generateWithOpenAIAndUsage(
  query: string,
  contextText: string
): Promise<{ answer: string; usage: TokenUsage }> {
  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: SYSTEM_PROMPT },
      { role: "user", content: `[Context]\n${contextText}\n\n[Question]\n${query}` }
    ],
    temperature: 0.1,
  });

  const answer = response.choices[0]?.message?.content || "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.";
  const usage = response.usage;

  return {
    answer,
    usage: {
      promptTokens: usage?.prompt_tokens || 0,
      completionTokens: usage?.completion_tokens || 0,
      totalTokens: usage?.total_tokens || 0
    }
  };
}
```

**ì²´í¬í¬ì¸íŠ¸ 5**:
- âœ… LLM API í˜¸ì¶œ ì„±ê³µ
- â±ï¸ í‰ê·  ì†Œìš” ì‹œê°„: 1000-3000ms (ê°€ì¥ í° ë³‘ëª©)
- âš ï¸ OpenAI gpt-4o ê¸°ì¤€

---

### Phase 6: ì´ë ¥ ì €ì¥ (ë¹„ë™ê¸°, 50-150ms)

```typescript
// 6. Supabaseì— Q&A ì´ë ¥ ì €ì¥ (non-blocking)
try {
  await saveQAHistory({
    session_id: sessionId,
    question,
    answer,
    category,
    sources,
    status,
    response_time_ms: responseTimeMs,
    // ... ê¸°íƒ€ ë©”íƒ€ë°ì´í„°
  });
} catch (dbError) {
  // ì‹¤íŒ¨í•´ë„ ì‘ë‹µ ë°˜í™˜ (ë¡œê·¸ë§Œ)
  console.warn('âš ï¸ Supabase ì €ì¥ ì‹¤íŒ¨:', dbError.message);
}
```

**ë¹„ë™ê¸° ì €ì¥ ì „ëµ**:
- âœ… ì €ì¥ ì‹¤íŒ¨í•´ë„ ì‚¬ìš©ì ì‘ë‹µì— ì˜í–¥ ì—†ìŒ
- âœ… ë¡œê·¸ ì†ì‹¤ ê°€ëŠ¥í•˜ì§€ë§Œ ì‚¬ìš©ì ê²½í—˜ ìš°ì„ 
- â±ï¸ í‰ê·  ì†Œìš” ì‹œê°„: 50-150ms

---

### Phase 7: ì‘ë‹µ ë°˜í™˜ (0-5ms)

```typescript
// 7. JSON ì‘ë‹µ
res.status(200).json({
  answer,
  sources,
  category,
  categoryConfidence: confidence,
  status,
  responseTimeMs,
  tokenUsage: usage.totalTokens,
  sessionId,

  timings: {
    classification: classificationTimeMs,
    vectorSearch: vectorSearchTimeMs,
    llmGeneration: llmGenerationTimeMs,
    dbSave: dbSaveTimeMs,
    total: responseTimeMs,
  },

  tokens: {
    prompt: usage.promptTokens,
    completion: usage.completionTokens,
    embedding: 0,
    total: usage.totalTokens,
  },
});
```

**ì²´í¬í¬ì¸íŠ¸ 7**:
- âœ… JSON ì§ë ¬í™” ë° ì „ì†¡
- â±ï¸ í‰ê·  ì†Œìš” ì‹œê°„: 0-5ms

---

### ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ë¶„ì„

```
Phase                     Cold Start    Warm Start
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. ìš”ì²­ ê²€ì¦              0-5ms         0-5ms
2. ì§ˆë¬¸ ë¶„ë¥˜              5-10ms        5-10ms
3. ì¿¼ë¦¬ ì„ë² ë”© (OpenAI)   100-300ms     100-300ms
4. ë²¡í„° ê²€ìƒ‰ (File)       150-380ms     51-151ms
   - íŒŒì¼ ë‹¤ìš´ë¡œë“œ        100-300ms     0ms (ìºì‹œ)
   - JSON íŒŒì‹±            20-50ms       0ms (ìºì‹œ)
   - ìœ ì‚¬ë„ ê³„ì‚°          30-30ms       51-151ms
5. LLM ìƒì„± (OpenAI)      1000-3000ms   1000-3000ms
6. ì´ë ¥ ì €ì¥ (Supabase)   50-150ms      50-150ms
7. ì‘ë‹µ ë°˜í™˜              0-5ms         0-5ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total                     1305-3850ms   1206-3621ms

í‰ê·  (Cold): ~2500ms (2.5ì´ˆ)
í‰ê·  (Warm): ~2400ms (2.4ì´ˆ)
```

**ë³‘ëª© êµ¬ê°„**:
1. **LLM ìƒì„±** (40-60%): 1000-3000ms â†’ ìµœì í™” ë¶ˆê°€ (ì™¸ë¶€ API)
2. **ì¿¼ë¦¬ ì„ë² ë”©** (5-10%): 100-300ms â†’ ìµœì í™” ë¶ˆê°€ (ì™¸ë¶€ API)
3. **ë²¡í„° ê²€ìƒ‰** (5-15%): 51-380ms â†’ ìºì‹±ìœ¼ë¡œ ìµœì í™” ê°€ëŠ¥ âœ…

---

## ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸

### 1. ë©”ëª¨ë¦¬ ìºì‹± (ê°€ì¥ ì¤‘ìš”)

**í˜„ì¬ êµ¬í˜„**:
```typescript
// src/service/vector-store/fileVectorStore.ts
let cachedIndex: VectorIndex | null = null;
let cacheTimestamp: number = 0;
const CACHE_TTL_MS = 5 * 60 * 1000; // 5ë¶„

async function loadVectorIndex(): Promise<VectorIndex> {
  // ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬
  if (cachedIndex && (now - cacheTimestamp) < CACHE_TTL_MS) {
    return cachedIndex; // Warm Start
  }

  // Cold Start: íŒŒì¼ ë‹¤ìš´ë¡œë“œ
  const response = await fetch(vectorFileUrl);
  // ...
  cachedIndex = index;
  cacheTimestamp = now;
  return index;
}
```

**íš¨ê³¼**:
- âœ… Cold Start: 150-380ms
- âœ… Warm Start: 51-151ms (60-75% ê°œì„ )

**Vercel Lambda ìºì‹± íŠ¹ì„±**:
- Lambda ì»¨í…Œì´ë„ˆëŠ” **5-15ë¶„** ë™ì•ˆ ì¬ì‚¬ìš©ë¨
- ë™ì¼ ë¦¬ì „ì—ì„œ ìš”ì²­ ì‹œ **80-90%** ìºì‹œ íˆíŠ¸ìœ¨
- ë©”ëª¨ë¦¬ ë³€ìˆ˜ (`let cachedIndex`)ëŠ” ì»¨í…Œì´ë„ˆê°€ ì‚´ì•„ìˆëŠ” ë™ì•ˆ ìœ ì§€

---

### 2. Gzip ì••ì¶• (íŒŒì¼ í¬ê¸° 60-70% ê°ì†Œ)

**ì••ì¶• íš¨ê³¼**:
```
embeddings.json:      7.5MB
embeddings.json.gz:   2.3MB (69% ê°ì†Œ)

ë‹¤ìš´ë¡œë“œ ì‹œê°„:
- ì••ì¶• ì „: 300-500ms (7.5MB @ 20Mbps)
- ì••ì¶• í›„: 100-150ms (2.3MB @ 20Mbps)

ì ˆê°: 200-350ms (50-70% ê°œì„ )
```

**êµ¬í˜„**:
```typescript
const buffer = Buffer.from(await response.arrayBuffer());

// Gzip ì••ì¶• í•´ì œ
if (vectorFileUrl.endsWith('.gz')) {
  const decompressed = await gunzipAsync(buffer);
  jsonString = decompressed.toString('utf-8');
}
```

---

### 3. CDN í™œìš© (Vercel Blob)

**CDN ì—†ì´** (Origin Server):
```
User (Seoul) â†’ Supabase (US West) â†’ 200-300ms latency
```

**CDN ì‚¬ìš©** (Vercel Edge):
```
User (Seoul) â†’ Vercel Edge (Seoul) â†’ 10-30ms latency
```

**ì„¤ì •**:
```bash
# Vercel Blobì— ì—…ë¡œë“œ
pnpm tsx scripts/export-embeddings.ts --source supabase --upload vercel

# ì¶œë ¥ëœ URL ì‚¬ìš©
VECTOR_FILE_URL=https://xxx.vercel-storage.com/embeddings.json.gz
```

**íš¨ê³¼**:
- âœ… ë‹¤ìš´ë¡œë“œ ì†ë„: 200-300ms â†’ 10-30ms (80-90% ê°œì„ )
- âœ… ê¸€ë¡œë²Œ ìºì‹œ (Edge Network)

---

### 4. ë©”íƒ€ë°ì´í„° í•„í„°ë§ ìµœì í™”

**í˜„ì¬ êµ¬í˜„** (ì„ í˜• ê²€ìƒ‰):
```typescript
for (const vec of index.vectors) {
  // ë©”íƒ€ë°ì´í„° í•„í„°
  if (filterMetadata) {
    const matches = Object.entries(filterMetadata).every(
      ([key, value]) => vec.metadata[key] === value
    );
    if (!matches) continue;
  }

  // ìœ ì‚¬ë„ ê³„ì‚°
  const score = cosineSimilarity(queryEmbedding, vec.embedding);
  // ...
}
```

**ê°œì„  ë°©ì•ˆ** (ì¸ë±ì‹±):
```typescript
// ë²¡í„° íŒŒì¼ì— owner/repoë³„ ì¸ë±ìŠ¤ ì¶”ê°€
interface VectorIndex {
  // ...
  byOwnerRepo: {
    [key: string]: number[]; // "owner/repo" â†’ vector indices
  };
}

// ê²€ìƒ‰ ì‹œ ì¸ë±ìŠ¤ í™œìš©
const key = `${owner}/${repo}`;
const indices = index.byOwnerRepo[key] || [];

for (const idx of indices) {
  const vec = index.vectors[idx];
  const score = cosineSimilarity(queryEmbedding, vec.embedding);
  // ...
}
```

**íš¨ê³¼**:
- âœ… í•„í„°ë§ ì‹œê°„: 30ms â†’ 5ms (83% ê°œì„ )
- âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„° (10,000+ vectors)ì—ì„œ íš¨ê³¼ì 

---

### 5. ë³‘ë ¬ ì²˜ë¦¬ (OpenAI API)

**í˜„ì¬** (ìˆœì°¨ ì²˜ë¦¬):
```typescript
const queryEmbedding = await generateQueryEmbedding(question); // 100-300ms
// ... ë²¡í„° ê²€ìƒ‰ ...
const { answer, usage } = await generateAnswerWithUsage(question, contexts); // 1000-3000ms
```

**ê°œì„  ë¶ˆê°€ ì´ìœ **:
- ë²¡í„° ê²€ìƒ‰ì€ ì¿¼ë¦¬ ì„ë² ë”©ì´ í•„ìš” (ìˆœì°¨ì  ì˜ì¡´ì„±)
- LLM ìƒì„±ì€ ê²€ìƒ‰ ê²°ê³¼(contexts)ê°€ í•„ìš” (ìˆœì°¨ì  ì˜ì¡´ì„±)

**ê°€ëŠ¥í•œ ë³‘ë ¬ ì²˜ë¦¬**:
```typescript
// ì§ˆë¬¸ ë¶„ë¥˜ì™€ ì„ë² ë”©ì„ ë³‘ë ¬ë¡œ
const [classification, queryEmbedding] = await Promise.all([
  Promise.resolve(classifyQuestionWithConfidence(question)), // ë™ê¸° í•¨ìˆ˜ë¥¼ Promiseë¡œ
  generateQueryEmbedding(question)
]);
```

**íš¨ê³¼**: ë¯¸ë¯¸ (5-10ms ê°œì„ )

---

### 6. LLM ìŠ¤íŠ¸ë¦¬ë° (ì‚¬ìš©ì ê²½í—˜ ê°œì„ )

**í˜„ì¬** (ì¼ê´„ ì‘ë‹µ):
```typescript
const { answer, usage } = await generateAnswerWithUsage(question, contexts);
res.json({ answer, ... });
```

**ê°œì„ ** (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ):
```typescript
res.setHeader('Content-Type', 'text/event-stream');
res.setHeader('Cache-Control', 'no-cache');
res.setHeader('Connection', 'keep-alive');

const stream = await openai.chat.completions.create({
  model: "gpt-4o",
  messages: [...],
  stream: true,
});

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || '';
  res.write(`data: ${JSON.stringify({ content })}\n\n`);
}

res.end();
```

**íš¨ê³¼**:
- âœ… ì²´ê° ì‘ë‹µ ì‹œê°„: 2500ms â†’ 500ms (ì²« ë‹¨ì–´ ì¶œë ¥)
- âœ… ì‚¬ìš©ì ê²½í—˜ ëŒ€í­ ê°œì„ 

---

## Serverless ì œí•œì‚¬í•­ ê³ ë ¤

### 1. ì‹¤í–‰ ì‹œê°„ ì œí•œ

**Vercel Limits**:
```
Hobby Plan:  60ì´ˆ (1ë¶„)
Pro Plan:    300ì´ˆ (5ë¶„)
```

**í˜„ì¬ í‰ê·  ì‹¤í–‰ ì‹œê°„**:
```
í‰ê· : 2.4ì´ˆ (4% ì‚¬ìš©)
ìµœëŒ€: 3.8ì´ˆ (6% ì‚¬ìš©)
```

**ì„¤ê³„ íŒë‹¨**:
- âœ… **ì•ˆì „**: í˜„ì¬ ì‹¤í–‰ ì‹œê°„ì€ ì œí•œì˜ 5% ë¯¸ë§Œ
- âœ… **ë²„í¼**: OpenAI APIê°€ ëŠë ¤ì ¸ë„ 60ì´ˆ ì•ˆì— ì™„ë£Œ
- âš ï¸ **ëª¨ë‹ˆí„°ë§ í•„ìš”**: LLMì´ 10ì´ˆ ì´ìƒ ê±¸ë¦´ ê²½ìš° íƒ€ì„ì•„ì›ƒ ìœ„í—˜

**íƒ€ì„ì•„ì›ƒ ë°©ì–´**:
```typescript
// vercel.json
{
  "functions": {
    "api/ask.ts": {
      "maxDuration": 60  // ëª…ì‹œì ìœ¼ë¡œ 60ì´ˆ ì„¤ì •
    }
  }
}
```

```typescript
// api/ask.ts
const TIMEOUT_MS = 55000; // 55ì´ˆ (5ì´ˆ ë²„í¼)

const timeoutPromise = new Promise((_, reject) =>
  setTimeout(() => reject(new Error('Function timeout')), TIMEOUT_MS)
);

try {
  const result = await Promise.race([
    generateAnswerWithUsage(question, contexts),
    timeoutPromise
  ]);
} catch (error) {
  if (error.message === 'Function timeout') {
    res.status(504).json({ error: 'Request timeout' });
    return;
  }
}
```

---

### 2. ë©”ëª¨ë¦¬ ì œí•œ

**Vercel Limits**:
```
Hobby Plan:  1024MB (1GB)
Pro Plan:    3008MB (3GB)
```

**í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**:
```
Vector Index (2.3MB gzip â†’ 7.5MB JSON):
- JSON íŒŒì‹±: ~7.5MB
- ê°ì²´ ë©”ëª¨ë¦¬: ~15MB (JavaScript overhead)
- ì´ ì‚¬ìš©ëŸ‰: ~30MB (3% ì‚¬ìš©)

Node.js Runtime: ~50MB
Total: ~80MB (8% ì‚¬ìš©)
```

**ëŒ€ìš©ëŸ‰ ë°ì´í„° ëŒ€ì‘** (10,000+ vectors):
```
10,000 vectors Ã— 1536 dimensions Ã— 4 bytes = 61MB (vectorsë§Œ)
+ Metadata: ~10MB
+ JSON ì˜¤ë²„í—¤ë“œ: ~30MB
Total: ~100MB (10% ì‚¬ìš©)
```

**ì„¤ê³„ íŒë‹¨**:
- âœ… **ì•ˆì „**: 1,000 vectorsê¹Œì§€ëŠ” ë¬¸ì œì—†ìŒ
- âš ï¸ **ì£¼ì˜**: 10,000 vectors ì´ìƒ ì‹œ ë©”ëª¨ë¦¬ ì••ë°•
- âŒ **í•œê³„**: 50,000 vectors ì´ìƒì€ ë©”ëª¨ë¦¬ ì´ˆê³¼ ìœ„í—˜

**ë©”ëª¨ë¦¬ ìµœì í™”**:
```typescript
// 1. Lazy loading (í•„ìš”í•œ ë¶€ë¶„ë§Œ ë¡œë”©)
interface VectorIndex {
  metadata: { count: number; dimension: number };
  vectors: VectorData[]; // ì „ì²´ ë¡œë”©í•˜ì§€ ì•ŠìŒ
}

// 2. ìŠ¤íŠ¸ë¦¼ íŒŒì‹± (ëŒ€ìš©ëŸ‰ JSON)
import { parse } from 'stream-json';
import { streamArray } from 'stream-json/streamers/StreamArray';

const stream = await fetch(vectorFileUrl).then(r => r.body);
const pipeline = stream.pipe(parse()).pipe(streamArray());

for await (const { value: vec } of pipeline) {
  // ë²¡í„° í•˜ë‚˜ì”© ì²˜ë¦¬
}
```

---

### 3. Cold Start ìµœì í™”

**Cold Start ë°œìƒ ì¡°ê±´**:
1. ì²« ë°°í¬ í›„ ì²« ìš”ì²­
2. 15ë¶„ ì´ìƒ ìš”ì²­ ì—†ìŒ
3. ë‹¤ë¥¸ ë¦¬ì „ì—ì„œ ìš”ì²­

**Cold Start ì‹œê°„**:
```
Lambda ì»¨í…Œì´ë„ˆ ë¶€íŒ…: 100-300ms
Node.js ì´ˆê¸°í™”: 50-100ms
íŒŒì¼ ë‹¤ìš´ë¡œë“œ (2.3MB): 100-150ms
JSON íŒŒì‹±: 20-50ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 270-600ms
```

**Warm Start ìœ ì§€ ì „ëµ**:
```typescript
// 1. Health check endpoint (5ë¶„ë§ˆë‹¤ í˜¸ì¶œ)
// api/health.ts
export default function handler(req: VercelRequest, res: VercelResponse) {
  res.status(200).json({ status: 'ok' });
}

// 2. Cron Job (ì™¸ë¶€ ì„œë¹„ìŠ¤)
// - UptimeRobot: 5ë¶„ë§ˆë‹¤ /api/health í˜¸ì¶œ
// - GitHub Actions: schedule ì‚¬ìš©
```

**ì„¤ê³„ íŒë‹¨**:
- âœ… **í—ˆìš©**: Cold StartëŠ” í”¼í•  ìˆ˜ ì—†ìŒ
- âœ… **ì™„í™”**: ìºì‹±ìœ¼ë¡œ Warm Start ë¹„ìœ¨ 80-90%
- âœ… **ì‚¬ìš©ì ê²½í—˜**: ì²« ìš”ì²­ ~3ì´ˆ, ì´í›„ ~2.4ì´ˆ

---

### 4. Stateless ì„¤ê³„ (ìƒíƒœ ë¹„ì €ì¥)

**Serverless íŠ¹ì„±**:
- í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤ê°€ ìš”ì²­ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
- ë©”ëª¨ë¦¬ ë³€ìˆ˜ëŠ” ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ì—ì„œë§Œ ìœ ì§€
- íŒŒì¼ ì‹œìŠ¤í…œ ì“°ê¸° ë¶ˆê°€ (`/tmp` ì œì™¸)

**ì„¤ê³„ íŒë‹¨**:
1. **ë©”ëª¨ë¦¬ ìºì‹œ**: í—ˆìš© (ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì¬ì‚¬ìš©)
   ```typescript
   let cachedIndex: VectorIndex | null = null; // âœ… OK
   ```

2. **ì„¸ì…˜ ê´€ë¦¬**: Supabaseì— ì €ì¥ (ì™¸ë¶€ ì €ì¥ì†Œ)
   ```typescript
   await saveQAHistory({ session_id, ... }); // âœ… OK
   ```

3. **íŒŒì¼ ì“°ê¸°**: ë¶ˆê°€ëŠ¥
   ```typescript
   fs.writeFileSync('data.json', '...'); // âŒ ì‘ë™ ì•ˆí•¨
   ```

4. **WebSocket**: ë¶ˆê°€ëŠ¥ (HTTPë§Œ ì§€ì›)
   ```typescript
   const ws = new WebSocket('ws://...'); // âŒ ì‘ë™ ì•ˆí•¨
   ```

**ì½ê¸° ì „ìš© ì„¤ê³„ì˜ ì´ì **:
- âœ… ì„ë² ë”© ìƒì„±ì€ CIì—ì„œë§Œ (GitHub Actions)
- âœ… APIëŠ” ì½ê¸°ë§Œ â†’ Stateless ì™„ë²½ í˜¸í™˜
- âœ… ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥ (Vercel Auto-scaling)

---

### 5. ë™ì‹œ ìš”ì²­ ì œí•œ

**Vercel Limits** (Hobby Plan):
```
ë™ì‹œ ì‹¤í–‰: 100ê°œ
ì´ˆê³¼ ì‹œ: 429 Too Many Requests
```

**ì˜ˆìƒ íŠ¸ë˜í”½**:
```
ì¼ì¼ ì‚¬ìš©ì: 100ëª…
ì‚¬ìš©ìë‹¹ ì§ˆë¬¸: 5ê°œ
ì´ ì§ˆë¬¸ ìˆ˜: 500ê°œ/ì¼

í‰ê·  ì‘ë‹µ ì‹œê°„: 2.4ì´ˆ
ë™ì‹œ ìš”ì²­ ìˆ˜: 500 / (24 * 60 * 60 / 2.4) â‰ˆ 0.014ê°œ

í”¼í¬ ì‹œê°„ (10ë°°): ~0.14ê°œ
```

**ì„¤ê³„ íŒë‹¨**:
- âœ… **ì•ˆì „**: ë™ì‹œ ìš”ì²­ 100ê°œ ì œí•œì€ ì¶©ë¶„
- âœ… **í™•ì¥ì„±**: Pro Plan (1000ê°œ)ë¡œ ì—…ê·¸ë ˆì´ë“œ ê°€ëŠ¥

---

## ë°°í¬ ë° ì„¤ì •

### 1. Vercel í”„ë¡œì íŠ¸ ìƒì„±

```bash
# 1. Vercel CLI ì„¤ì¹˜
npm i -g vercel

# 2. í”„ë¡œì íŠ¸ ë°°í¬
vercel

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Vercel Dashboard)
# Settings â†’ Environment Variables

VECTOR_FILE_URL=https://xxx.vercel-storage.com/embeddings.json.gz
OPENAI_API_KEY=sk-proj-xxx
CLAUDE_API_KEY=sk-ant-xxx
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=xxx
TARGET_REPO_OWNER=username
TARGET_REPO_NAME=repo-name
```

---

### 2. vercel.json ì„¤ì •

```json
{
  "version": 2,
  "name": "nlp-portfolio-api",
  "builds": [
    {
      "src": "src/service/server/index.ts",
      "use": "@vercel/node"
    }
  ],
  "functions": {
    "api/ask.ts": {
      "memory": 1024,
      "maxDuration": 60
    }
  },
  "headers": [
    {
      "source": "/api/(.*)",
      "headers": [
        { "key": "Access-Control-Allow-Origin", "value": "*" },
        { "key": "Access-Control-Allow-Methods", "value": "GET,POST,OPTIONS" }
      ]
    }
  ]
}
```

---

### 3. íŒŒì¼ êµ¬ì¡°

```
project/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ ask.ts              # Serverless function (ìë™ ë¼ìš°íŒ…)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”œâ”€â”€ vector-store/
â”‚   â”‚   â”‚   â”œâ”€â”€ fileVectorStore.ts
â”‚   â”‚   â”‚   â””â”€â”€ embeddingService.ts
â”‚   â”‚   â””â”€â”€ qa/
â”‚   â”‚       â””â”€â”€ answer.ts
â”‚   â””â”€â”€ ...
â”œâ”€â”€ vercel.json             # Vercel ì„¤ì •
â””â”€â”€ package.json
```

---

## ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

### 1. Vercel Analytics

**ê¸°ë³¸ ë©”íŠ¸ë¦­**:
- **Execution Duration**: í‰ê·  2.4ì´ˆ ëª©í‘œ
- **Memory Usage**: 80MB ë¯¸ë§Œ ëª©í‘œ
- **Invocations**: ì¼ì¼ 500ê°œ ì˜ˆìƒ
- **Error Rate**: 1% ë¯¸ë§Œ ëª©í‘œ

**ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**:
```
Vercel Dashboard â†’ Project â†’ Analytics

- P50: 2.2ì´ˆ
- P95: 3.5ì´ˆ
- P99: 4.0ì´ˆ
- Error Rate: 0.5%
```

---

### 2. ì„±ëŠ¥ ë¡œê¹…

```typescript
// api/ask.ts
console.log(`ğŸ“Š Timings:
  Classification: ${classificationTimeMs}ms
  Vector Search: ${vectorSearchTimeMs}ms
  LLM Generation: ${llmGenerationTimeMs}ms
  DB Save: ${dbSaveTimeMs}ms
  Total: ${responseTimeMs}ms
`);
```

**Vercel Logs**:
```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
vercel logs --follow

# íŠ¹ì • í•¨ìˆ˜ ë¡œê·¸
vercel logs api/ask.ts
```

---

### 3. ë¹„ìš© ëª¨ë‹ˆí„°ë§

**Vercel Serverless (Hobby Plan)**:
```
ë¬´ë£Œ tier:
- Invocations: 100GB-Hours/ì›”
- Bandwidth: 100GB/ì›”

ì˜ˆìƒ ì‚¬ìš©ëŸ‰ (ì›” 500 ìš”ì²­ ê¸°ì¤€):
- Invocations: 500 Ã— 2.4ì´ˆ Ã— 1GB = 0.33GB-Hours (0.3% ì‚¬ìš©)
- Bandwidth: 500 Ã— 5KB = 2.5MB (0.0025% ì‚¬ìš©)

Total: $0/ì›” (ë¬´ë£Œ)
```

**OpenAI API**:
```
Embedding (text-embedding-3-small):
- 500 queries Ã— 50 tokens = 25,000 tokens
- $0.020 / 1M tokens
- Cost: $0.0005/ì›”

Generation (gpt-4o):
- 500 queries Ã— 2000 tokens (avg) = 1M tokens
- $5 / 1M input tokens, $15 / 1M output tokens
- Input: $5 Ã— 0.7 = $3.5
- Output: $15 Ã— 0.3 = $4.5
- Cost: $8/ì›”

Total: ~$8/ì›”
```

---

## ìš”ì•½

### âœ… ì„¤ê³„ ëª©í‘œ ë‹¬ì„±

| ëª©í‘œ | ë‹¬ì„± ì—¬ë¶€ | ì„¸ë¶€ ì‚¬í•­ |
|------|-----------|----------|
| ì½ê¸° ì „ìš© | âœ… | ì„ë² ë”©ì€ CIì—ì„œ ìƒì„±, APIëŠ” ì½ê¸°ë§Œ |
| ë¹ ë¥¸ ì‘ë‹µ | âœ… | í‰ê·  2.4ì´ˆ (Warm Start) |
| ìƒíƒœ ë¹„ì €ì¥ | âœ… | ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©, ì™¸ë¶€ ì €ì¥ì†Œ í™œìš© |
| ì„œë²„ ë¹„ìš© $0 | âœ… | Vercel ë¬´ë£Œ tier í™œìš© |

### ğŸ¯ í•µì‹¬ ìµœì í™”

1. **ë©”ëª¨ë¦¬ ìºì‹±**: Cold Start 380ms â†’ Warm Start 51ms (86% ê°œì„ )
2. **Gzip ì••ì¶•**: íŒŒì¼ í¬ê¸° 7.5MB â†’ 2.3MB (69% ê°ì†Œ)
3. **CDN í™œìš©**: ë‹¤ìš´ë¡œë“œ 300ms â†’ 30ms (90% ê°œì„ )
4. **ë¹„ë™ê¸° ë¡œê¹…**: Supabase ì €ì¥ ì‹¤íŒ¨í•´ë„ ì‘ë‹µ ë°˜í™˜

### ğŸ“Š ì„±ëŠ¥ ì§€í‘œ

```
Cold Start: 2500ms (ì²« ìš”ì²­)
Warm Start: 2400ms (ìºì‹œ íˆíŠ¸)

ë³‘ëª© êµ¬ê°„:
1. LLM ìƒì„±: 1000-3000ms (60%)
2. ì¿¼ë¦¬ ì„ë² ë”©: 100-300ms (12%)
3. ë²¡í„° ê²€ìƒ‰: 51-151ms (6%)

ë¹„ìš©:
- Vercel: $0/ì›” (ë¬´ë£Œ tier)
- OpenAI: ~$8/ì›” (500 queries)
- Total: ~$8/ì›”
```

---

**ì‘ì„±ì¼**: 2025-12-31
**ë²„ì „**: 1.0.0
**ê´€ë ¨ ë¬¸ì„œ**: [CI-AUTOMATION.md](./CI-AUTOMATION.md), [FILE-BASED-VECTOR-STORE.md](./FILE-BASED-VECTOR-STORE.md)
